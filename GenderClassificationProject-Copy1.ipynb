{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification from Text Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!  conda install  openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_excel('blog-gender-dataset/blog-gender-dataset.xlsx', engine='openpyxl')\n",
    "headers = [\"text\", \"class\"]\n",
    "text_df = pd.read_csv('blog-gender-dataset/blog-gender-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3231 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.iloc[:, [0, 1]]\n",
    "text_df.columns = headers\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3231, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3226\n",
       "unique       8\n",
       "top          M\n",
       "freq      1546\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"class\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3231 entries, 0 to 3230\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3224 non-null   object\n",
      " 1   class   3226 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 50.6+ KB\n"
     ]
    }
   ],
   "source": [
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1546, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"M\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"F\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3224 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3224 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.dropna()\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    1677\n",
       "F    1547\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"class\"] = text_df[\"class\"].replace(' F', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('f', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('F ', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace(' M', 'M')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('m', 'M')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace(' M ', 'M')\n",
    "text_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df1 = pd.read_csv('data/blogtext.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df1 = text_df1[[\"text\",\"gender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df1.columns = headers\n",
    "text_df1[\"class\"] = text_df1[\"class\"].replace('female', 'F')\n",
    "text_df1[\"class\"] = text_df1[\"class\"].replace('male', 'M')\n",
    "text_df1 = text_df1[text_df1[\"text\"]!= \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_M = [text_df1[text_df1[\"class\"]==\"M\"], text_df2[text_df2[\"class\"]==\"M\"]]\n",
    "df_M = [text_df1[text_df1[\"class\"]==\"M\"]]\n",
    "df_M = pd.concat(df_M)\n",
    "df_M = df_M.sample(frac=1).reset_index(drop=True)\n",
    "#df_F = [text_df1[text_df1[\"class\"]==\"F\"], text_df2[text_df2[\"class\"]==\"F\"]]\n",
    "df_F = [text_df1[text_df1[\"class\"]==\"F\"]]\n",
    "df_F = pd.concat(df_F)\n",
    "df_F = df_F.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    100000\n",
       "M    100000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_df = [text_df, df_M[:30000], df_F[:30000]]\n",
    "text_df = [ df_M[:100000], df_F[:100000]]\n",
    "text_df = pd.concat(text_df)\n",
    "text_df = text_df.sample(frac=1).reset_index(drop=True)\n",
    "text_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                 I just realized that I haven't posted anything worthwhile in a LONG  TIME~  So I will try to post something.   Hm.I get my hair cut tomorrow. I am soooo nervous!!!! I hope they don't mess up or anything. But if they do its ok, because its just hair. It will grow!   Today was kinda a yucky day. It was just dull and I did absolutely nothign worthwile. I made 4 smoothies. Haha.  UGH!!! *tries to think of something worthwhile to post*  Well, I get my hair cut. I am quite nervous. Wait! I already posted about this. Haha.  Hmm..tonight I am going to try to go to this Sara thinng :)   I tyhink its raining outside which means we wont have a game today :( waaaa  I really wanted to play!  Yeah. this attempt is quite worthless..I betetr go get ready for tennis, bye!  Laura         \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \" \".join([c for c in text if c not in punctuations])\n",
    "    return no_punct\n",
    "#textclean = text_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "#instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#textclean = textclean.apply(lambda x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lazee. Lazy. Loused Up. I can't eve...</td>\n",
       "      <td>F</td>\n",
       "      <td>lazee lazy loused up i cant even be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just realized that I haven'...</td>\n",
       "      <td>F</td>\n",
       "      <td>i just realized that i havent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am updated my blog because&amp;nbsp...</td>\n",
       "      <td>M</td>\n",
       "      <td>i am updated my blog becausenbsp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guys  at Rogers finally ...</td>\n",
       "      <td>M</td>\n",
       "      <td>the guys  at rogers finally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tapping is finished! yay! now...</td>\n",
       "      <td>F</td>\n",
       "      <td>tapping is finished yay now w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0             Lazee. Lazy. Loused Up. I can't eve...     F   \n",
       "1                   I just realized that I haven'...     F   \n",
       "2               i am updated my blog because&nbsp...     M   \n",
       "3                    The guys  at Rogers finally ...     M   \n",
       "4                   Tapping is finished! yay! now...     F   \n",
       "\n",
       "                                          text_clean  \n",
       "0             lazee lazy loused up i cant even be...  \n",
       "1                   i just realized that i havent...  \n",
       "2               i am updated my blog becausenbsp ...  \n",
       "3                    the guys  at rogers finally ...  \n",
       "4                   tapping is finished yay now w...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n",
    "data_clean = clean_text(text_df, 'text', 'text_clean')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lazee. Lazy. Loused Up. I can't eve...</td>\n",
       "      <td>F</td>\n",
       "      <td>lazee lazy loused cant even begin tell think m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just realized that I haven'...</td>\n",
       "      <td>F</td>\n",
       "      <td>realized havent posted anything worthwhile lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am updated my blog because&amp;nbsp...</td>\n",
       "      <td>M</td>\n",
       "      <td>updated blog becausenbsp siu kwok hanzi update...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guys  at Rogers finally ...</td>\n",
       "      <td>M</td>\n",
       "      <td>guys rogers finally figured solved slow intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tapping is finished! yay! now...</td>\n",
       "      <td>F</td>\n",
       "      <td>tapping finished yay get boil laura aka cannibal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0             Lazee. Lazy. Loused Up. I can't eve...     F   \n",
       "1                   I just realized that I haven'...     F   \n",
       "2               i am updated my blog because&nbsp...     M   \n",
       "3                    The guys  at Rogers finally ...     M   \n",
       "4                   Tapping is finished! yay! now...     F   \n",
       "\n",
       "                                          text_clean  \n",
       "0  lazee lazy loused cant even begin tell think m...  \n",
       "1  realized havent posted anything worthwhile lon...  \n",
       "2  updated blog becausenbsp siu kwok hanzi update...  \n",
       "3  guys rogers finally figured solved slow intern...  \n",
       "4   tapping finished yay get boil laura aka cannibal  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk.corpus\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lazee lazy loused cant even begin tell think missing tropfest somewhat something bothering fucking bashed trashed last nights housewarming party know travel oxford st sit super films well count urllink paradisco wasnt super mmwhat saying sound like know im talking came back really wasnt mood im also really bad feelings paranoia freaking like want time alone goddammit nurse fried braincells peace impatient reading things online impatient peoples lack consideration art man mean art want see beautiful breathless yet people would rather repeat crap pretending repeating hmm war comment dont think loz said schmidt mind howard stupid nazi incline agree lets talk party lot people came favorite guest loz fucking sweetheart really need talk thank much organise make comfortable house little time im lazy lazeee like example putting speedial phone fucked terminally stoned took many drugs last night first time life got terrified chems man didnt want sniff anything like get away dont know brain copes weed constant stress feel like im limbo stage embrace drugs forever say fuck stay socially see im going really addicted dont want go chems really mean really take another level desperation know cb line e smoked dope drunk like little fucking fishes blast talking cockshit lost world music thought nice ride fuck man sometimes want calm ride say want forever conservative dont want left nothing verbal diarreah lodged brain whats point cant remember scenic ride thats learn shit right'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['text_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lazee. Lazy. Loused Up. I can't eve...</td>\n",
       "      <td>F</td>\n",
       "      <td>lazee lazy loused cant even begin tell think m...</td>\n",
       "      <td>[lazee, lazy, loused, cant, even, begin, tell,...</td>\n",
       "      <td>[laze, lazi, lous, cant, even, begin, tell, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just realized that I haven'...</td>\n",
       "      <td>F</td>\n",
       "      <td>realized havent posted anything worthwhile lon...</td>\n",
       "      <td>[realized, havent, posted, anything, worthwhil...</td>\n",
       "      <td>[realiz, havent, post, anyth, worthwhil, long,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am updated my blog because&amp;nbsp...</td>\n",
       "      <td>M</td>\n",
       "      <td>updated blog becausenbsp siu kwok hanzi update...</td>\n",
       "      <td>[updated, blog, becausenbsp, siu, kwok, hanzi,...</td>\n",
       "      <td>[updat, blog, becausenbsp, siu, kwok, hanzi, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guys  at Rogers finally ...</td>\n",
       "      <td>M</td>\n",
       "      <td>guys rogers finally figured solved slow intern...</td>\n",
       "      <td>[guys, rogers, finally, figured, solved, slow,...</td>\n",
       "      <td>[guy, roger, final, figur, solv, slow, interne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tapping is finished! yay! now...</td>\n",
       "      <td>F</td>\n",
       "      <td>tapping finished yay get boil laura aka cannibal</td>\n",
       "      <td>[tapping, finished, yay, get, boil, laura, aka...</td>\n",
       "      <td>[tap, finish, yay, get, boil, laura, aka, cannib]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0             Lazee. Lazy. Loused Up. I can't eve...     F   \n",
       "1                   I just realized that I haven'...     F   \n",
       "2               i am updated my blog because&nbsp...     M   \n",
       "3                    The guys  at Rogers finally ...     M   \n",
       "4                   Tapping is finished! yay! now...     F   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  lazee lazy loused cant even begin tell think m...   \n",
       "1  realized havent posted anything worthwhile lon...   \n",
       "2  updated blog becausenbsp siu kwok hanzi update...   \n",
       "3  guys rogers finally figured solved slow intern...   \n",
       "4   tapping finished yay get boil laura aka cannibal   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [lazee, lazy, loused, cant, even, begin, tell,...   \n",
       "1  [realized, havent, posted, anything, worthwhil...   \n",
       "2  [updated, blog, becausenbsp, siu, kwok, hanzi,...   \n",
       "3  [guys, rogers, finally, figured, solved, slow,...   \n",
       "4  [tapping, finished, yay, get, boil, laura, aka...   \n",
       "\n",
       "                                    text_tokens_stem  \n",
       "0  [laze, lazi, lous, cant, even, begin, tell, th...  \n",
       "1  [realiz, havent, post, anyth, worthwhil, long,...  \n",
       "2  [updat, blog, becausenbsp, siu, kwok, hanzi, u...  \n",
       "3  [guy, roger, final, figur, solv, slow, interne...  \n",
       "4  [tap, finish, yay, get, boil, laura, aka, cannib]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk \n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lazee. Lazy. Loused Up. I can't eve...</td>\n",
       "      <td>F</td>\n",
       "      <td>lazee lazy loused cant even begin tell think m...</td>\n",
       "      <td>[lazee, lazy, loused, cant, even, begin, tell,...</td>\n",
       "      <td>[laze, lazi, lous, cant, even, begin, tell, th...</td>\n",
       "      <td>[lazee, lazy, loused, cant, even, begin, tell,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just realized that I haven'...</td>\n",
       "      <td>F</td>\n",
       "      <td>realized havent posted anything worthwhile lon...</td>\n",
       "      <td>[realized, havent, posted, anything, worthwhil...</td>\n",
       "      <td>[realiz, havent, post, anyth, worthwhil, long,...</td>\n",
       "      <td>[realized, havent, posted, anything, worthwhil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am updated my blog because&amp;nbsp...</td>\n",
       "      <td>M</td>\n",
       "      <td>updated blog becausenbsp siu kwok hanzi update...</td>\n",
       "      <td>[updated, blog, becausenbsp, siu, kwok, hanzi,...</td>\n",
       "      <td>[updat, blog, becausenbsp, siu, kwok, hanzi, u...</td>\n",
       "      <td>[updated, blog, becausenbsp, siu, kwok, hanzi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guys  at Rogers finally ...</td>\n",
       "      <td>M</td>\n",
       "      <td>guys rogers finally figured solved slow intern...</td>\n",
       "      <td>[guys, rogers, finally, figured, solved, slow,...</td>\n",
       "      <td>[guy, roger, final, figur, solv, slow, interne...</td>\n",
       "      <td>[guy, rogers, finally, figured, solved, slow, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tapping is finished! yay! now...</td>\n",
       "      <td>F</td>\n",
       "      <td>tapping finished yay get boil laura aka cannibal</td>\n",
       "      <td>[tapping, finished, yay, get, boil, laura, aka...</td>\n",
       "      <td>[tap, finish, yay, get, boil, laura, aka, cannib]</td>\n",
       "      <td>[tapping, finished, yay, get, boil, laura, aka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0             Lazee. Lazy. Loused Up. I can't eve...     F   \n",
       "1                   I just realized that I haven'...     F   \n",
       "2               i am updated my blog because&nbsp...     M   \n",
       "3                    The guys  at Rogers finally ...     M   \n",
       "4                   Tapping is finished! yay! now...     F   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  lazee lazy loused cant even begin tell think m...   \n",
       "1  realized havent posted anything worthwhile lon...   \n",
       "2  updated blog becausenbsp siu kwok hanzi update...   \n",
       "3  guys rogers finally figured solved slow intern...   \n",
       "4   tapping finished yay get boil laura aka cannibal   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [lazee, lazy, loused, cant, even, begin, tell,...   \n",
       "1  [realized, havent, posted, anything, worthwhil...   \n",
       "2  [updated, blog, becausenbsp, siu, kwok, hanzi,...   \n",
       "3  [guys, rogers, finally, figured, solved, slow,...   \n",
       "4  [tapping, finished, yay, get, boil, laura, aka...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [laze, lazi, lous, cant, even, begin, tell, th...   \n",
       "1  [realiz, havent, post, anyth, worthwhil, long,...   \n",
       "2  [updat, blog, becausenbsp, siu, kwok, hanzi, u...   \n",
       "3  [guy, roger, final, figur, solv, slow, interne...   \n",
       "4  [tap, finish, yay, get, boil, laura, aka, cannib]   \n",
       "\n",
       "                                   text_tokens_lemma  \n",
       "0  [lazee, lazy, loused, cant, even, begin, tell,...  \n",
       "1  [realized, havent, posted, anything, worthwhil...  \n",
       "2  [updated, blog, becausenbsp, siu, kwok, hanzi,...  \n",
       "3  [guy, rogers, finally, figured, solved, slow, ...  \n",
       "4  [tapping, finished, yay, get, boil, laura, aka...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lazee. Lazy. Loused Up. I can't eve...</td>\n",
       "      <td>F</td>\n",
       "      <td>lazee lazy loused cant even begin tell think m...</td>\n",
       "      <td>[lazee, lazy, loused, cant, even, begin, tell,...</td>\n",
       "      <td>[laze, lazi, lous, cant, even, begin, tell, th...</td>\n",
       "      <td>[lazee, lazy, loused, cant, even, begin, tell,...</td>\n",
       "      <td>[(lazee, NN), (lazy, NN), (loused, VBD), (cant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just realized that I haven'...</td>\n",
       "      <td>F</td>\n",
       "      <td>realized havent posted anything worthwhile lon...</td>\n",
       "      <td>[realized, havent, posted, anything, worthwhil...</td>\n",
       "      <td>[realiz, havent, post, anyth, worthwhil, long,...</td>\n",
       "      <td>[realized, havent, posted, anything, worthwhil...</td>\n",
       "      <td>[(realized, VBN), (havent, NN), (posted, VBD),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am updated my blog because&amp;nbsp...</td>\n",
       "      <td>M</td>\n",
       "      <td>updated blog becausenbsp siu kwok hanzi update...</td>\n",
       "      <td>[updated, blog, becausenbsp, siu, kwok, hanzi,...</td>\n",
       "      <td>[updat, blog, becausenbsp, siu, kwok, hanzi, u...</td>\n",
       "      <td>[updated, blog, becausenbsp, siu, kwok, hanzi,...</td>\n",
       "      <td>[(updated, VBN), (blog, NN), (becausenbsp, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The guys  at Rogers finally ...</td>\n",
       "      <td>M</td>\n",
       "      <td>guys rogers finally figured solved slow intern...</td>\n",
       "      <td>[guys, rogers, finally, figured, solved, slow,...</td>\n",
       "      <td>[guy, roger, final, figur, solv, slow, interne...</td>\n",
       "      <td>[guy, rogers, finally, figured, solved, slow, ...</td>\n",
       "      <td>[(guys, NNS), (rogers, NNS), (finally, RB), (f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tapping is finished! yay! now...</td>\n",
       "      <td>F</td>\n",
       "      <td>tapping finished yay get boil laura aka cannibal</td>\n",
       "      <td>[tapping, finished, yay, get, boil, laura, aka...</td>\n",
       "      <td>[tap, finish, yay, get, boil, laura, aka, cannib]</td>\n",
       "      <td>[tapping, finished, yay, get, boil, laura, aka...</td>\n",
       "      <td>[(tapping, VBG), (finished, VBN), (yay, RB), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0             Lazee. Lazy. Loused Up. I can't eve...     F   \n",
       "1                   I just realized that I haven'...     F   \n",
       "2               i am updated my blog because&nbsp...     M   \n",
       "3                    The guys  at Rogers finally ...     M   \n",
       "4                   Tapping is finished! yay! now...     F   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  lazee lazy loused cant even begin tell think m...   \n",
       "1  realized havent posted anything worthwhile lon...   \n",
       "2  updated blog becausenbsp siu kwok hanzi update...   \n",
       "3  guys rogers finally figured solved slow intern...   \n",
       "4   tapping finished yay get boil laura aka cannibal   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [lazee, lazy, loused, cant, even, begin, tell,...   \n",
       "1  [realized, havent, posted, anything, worthwhil...   \n",
       "2  [updated, blog, becausenbsp, siu, kwok, hanzi,...   \n",
       "3  [guys, rogers, finally, figured, solved, slow,...   \n",
       "4  [tapping, finished, yay, get, boil, laura, aka...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [laze, lazi, lous, cant, even, begin, tell, th...   \n",
       "1  [realiz, havent, post, anyth, worthwhil, long,...   \n",
       "2  [updat, blog, becausenbsp, siu, kwok, hanzi, u...   \n",
       "3  [guy, roger, final, figur, solv, slow, interne...   \n",
       "4  [tap, finish, yay, get, boil, laura, aka, cannib]   \n",
       "\n",
       "                                   text_tokens_lemma  \\\n",
       "0  [lazee, lazy, loused, cant, even, begin, tell,...   \n",
       "1  [realized, havent, posted, anything, worthwhil...   \n",
       "2  [updated, blog, becausenbsp, siu, kwok, hanzi,...   \n",
       "3  [guy, rogers, finally, figured, solved, slow, ...   \n",
       "4  [tapping, finished, yay, get, boil, laura, aka...   \n",
       "\n",
       "                              text_tokens_pos_tagged  \n",
       "0  [(lazee, NN), (lazy, NN), (loused, VBD), (cant...  \n",
       "1  [(realized, VBN), (havent, NN), (posted, VBD),...  \n",
       "2  [(updated, VBN), (blog, NN), (becausenbsp, NN)...  \n",
       "3  [(guys, NNS), (rogers, NNS), (finally, RB), (f...  \n",
       "4  [(tapping, VBG), (finished, VBN), (yay, RB), (...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of speech tagging and chunking\n",
    "def word_pos_tagger(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    return pos_tagged_text\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_clean['text_tokens_pos_tagged'] = data_clean['text_tokens'].apply(lambda x: word_pos_tagger(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "      <th>fine</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114085</th>\n",
       "      <td>Lava lamps are awesome Even more so the...</td>\n",
       "      <td>M</td>\n",
       "      <td>lava lamps awesome even possum made wax oil u ...</td>\n",
       "      <td>[lava, lamps, awesome, even, possum, made, wax...</td>\n",
       "      <td>[lava, lamp, awesom, even, possum, made, wax, ...</td>\n",
       "      <td>[lava, lamp, awesome, even, possum, made, wax,...</td>\n",
       "      <td>[(lava, NN), (lamps, VBZ), (awesome, JJ), (eve...</td>\n",
       "      <td>lava lamp awesome even possum made leave long ...</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149143</th>\n",
       "      <td>Poet-seers who have achieved the hi...</td>\n",
       "      <td>M</td>\n",
       "      <td>poetseers achieved highest realisations endeav...</td>\n",
       "      <td>[poetseers, achieved, highest, realisations, e...</td>\n",
       "      <td>[poetseer, achiev, highest, realis, endeavour,...</td>\n",
       "      <td>[poetseers, achieved, highest, realisation, en...</td>\n",
       "      <td>[(poetseers, NNS), (achieved, VBD), (highest, ...</td>\n",
       "      <td>poetseers achieved highest realisation endeavo...</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196033</th>\n",
       "      <td>You're so gay.  (filled with love.)...</td>\n",
       "      <td>F</td>\n",
       "      <td>youre gay filled love</td>\n",
       "      <td>[youre, gay, filled, love]</td>\n",
       "      <td>[your, gay, fill, love]</td>\n",
       "      <td>[youre, gay, filled, love]</td>\n",
       "      <td>[(youre, NN), (gay, NN), (filled, VBN), (love,...</td>\n",
       "      <td>youre filled love</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70772</th>\n",
       "      <td>THERE'S SOMETHING DEEPLY SURR...</td>\n",
       "      <td>M</td>\n",
       "      <td>theres something deeply surreal introducing bl...</td>\n",
       "      <td>[theres, something, deeply, surreal, introduci...</td>\n",
       "      <td>[there, someth, deepli, surreal, introduc, blo...</td>\n",
       "      <td>[there, something, deeply, surreal, introducin...</td>\n",
       "      <td>[(theres, NNS), (something, NN), (deeply, RB),...</td>\n",
       "      <td>there something deeply surreal introducing blo...</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156066</th>\n",
       "      <td>Have you ever had one of those days whe...</td>\n",
       "      <td>F</td>\n",
       "      <td>ever one days want throttle one coworkers one ...</td>\n",
       "      <td>[ever, one, days, want, throttle, one, coworke...</td>\n",
       "      <td>[ever, one, day, want, throttl, one, cowork, o...</td>\n",
       "      <td>[ever, one, day, want, throttle, one, coworker...</td>\n",
       "      <td>[(ever, RB), (one, CD), (days, NNS), (want, VB...</td>\n",
       "      <td>ever want throttle coworkers today went work g...</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181932</th>\n",
       "      <td>omg today i .... am gonna do everything...</td>\n",
       "      <td>F</td>\n",
       "      <td>omg today gonna everything dont wanna woohoo i...</td>\n",
       "      <td>[omg, today, gon, na, everything, dont, wan, n...</td>\n",
       "      <td>[omg, today, gon, na, everyth, dont, wan, na, ...</td>\n",
       "      <td>[omg, today, gon, na, everything, dont, wan, n...</td>\n",
       "      <td>[(omg, NN), (today, NN), (gon, VB), (na, TO), ...</td>\n",
       "      <td>today everything dont woohoo damn excited last...</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30928</th>\n",
       "      <td>so today was one of the best ...</td>\n",
       "      <td>F</td>\n",
       "      <td>today one best workouts ive well long time tha...</td>\n",
       "      <td>[today, one, best, workouts, ive, well, long, ...</td>\n",
       "      <td>[today, one, best, workout, ive, well, long, t...</td>\n",
       "      <td>[today, one, best, workout, ive, well, long, t...</td>\n",
       "      <td>[(today, NN), (one, CD), (best, JJS), (workout...</td>\n",
       "      <td>today best workout well long time thats good m...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47022</th>\n",
       "      <td>hi dideley ho neighbour!</td>\n",
       "      <td>M</td>\n",
       "      <td>hi dideley ho neighbour</td>\n",
       "      <td>[hi, dideley, ho, neighbour]</td>\n",
       "      <td>[hi, dideley, ho, neighbour]</td>\n",
       "      <td>[hi, dideley, ho, neighbour]</td>\n",
       "      <td>[(hi, NN), (dideley, NN), (ho, VBD), (neighbou...</td>\n",
       "      <td>dideley neighbour</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23080</th>\n",
       "      <td>Pursuant to the last post,  urlLink Jus...</td>\n",
       "      <td>F</td>\n",
       "      <td>pursuant last post urllink desserts cream liqu...</td>\n",
       "      <td>[pursuant, last, post, urllink, desserts, crea...</td>\n",
       "      <td>[pursuant, last, post, urllink, dessert, cream...</td>\n",
       "      <td>[pursuant, last, post, urllink, dessert, cream...</td>\n",
       "      <td>[(pursuant, NN), (last, JJ), (post, NN), (urll...</td>\n",
       "      <td>pursuant last post urllink dessert cream lique...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50642</th>\n",
       "      <td>As I stare into the depths of spa...</td>\n",
       "      <td>M</td>\n",
       "      <td>stare depths space worry stricken nook thought...</td>\n",
       "      <td>[stare, depths, space, worry, stricken, nook, ...</td>\n",
       "      <td>[stare, depth, space, worri, stricken, nook, t...</td>\n",
       "      <td>[stare, depth, space, worry, stricken, nook, t...</td>\n",
       "      <td>[(stare, JJ), (depths, NNS), (space, NN), (wor...</td>\n",
       "      <td>stare depth space worry stricken nook thought ...</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157153</th>\n",
       "      <td>Me and chandru when he come to bang...</td>\n",
       "      <td>M</td>\n",
       "      <td>chandru come banglore june</td>\n",
       "      <td>[chandru, come, banglore, june]</td>\n",
       "      <td>[chandru, come, banglor, june]</td>\n",
       "      <td>[chandru, come, banglore, june]</td>\n",
       "      <td>[(chandru, NN), (come, VBN), (banglore, IN), (...</td>\n",
       "      <td>chandru come banglore june</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106654</th>\n",
       "      <td>I know Mel Dobberteen from high sch...</td>\n",
       "      <td>M</td>\n",
       "      <td>know mel dobberteen high school mel bet bucks ...</td>\n",
       "      <td>[know, mel, dobberteen, high, school, mel, bet...</td>\n",
       "      <td>[know, mel, dobberteen, high, school, mel, bet...</td>\n",
       "      <td>[know, mel, dobberteen, high, school, mel, bet...</td>\n",
       "      <td>[(know, VB), (mel, NN), (dobberteen, JJ), (hig...</td>\n",
       "      <td>know dobberteen high school buck head first ma...</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182925</th>\n",
       "      <td>Well, some of the heavy hitters were up...</td>\n",
       "      <td>M</td>\n",
       "      <td>well heavy hitters last night demcon al gore s...</td>\n",
       "      <td>[well, heavy, hitters, last, night, demcon, al...</td>\n",
       "      <td>[well, heavi, hitter, last, night, demcon, al,...</td>\n",
       "      <td>[well, heavy, hitter, last, night, demcon, al,...</td>\n",
       "      <td>[(well, RB), (heavy, JJ), (hitters, NNS), (las...</td>\n",
       "      <td>well heavy hitter last night demcon gore start...</td>\n",
       "      <td>2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182661</th>\n",
       "      <td>(Click the link above.)  Its so s...</td>\n",
       "      <td>M</td>\n",
       "      <td>click link stupid isnt funny well maybe little...</td>\n",
       "      <td>[click, link, stupid, isnt, funny, well, maybe...</td>\n",
       "      <td>[click, link, stupid, isnt, funni, well, mayb,...</td>\n",
       "      <td>[click, link, stupid, isnt, funny, well, maybe...</td>\n",
       "      <td>[(click, NN), (link, NN), (stupid, JJ), (isnt,...</td>\n",
       "      <td>click link stupid isnt funny well maybe little...</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150595</th>\n",
       "      <td>Excellent stuff, Bennett. Photoshop...</td>\n",
       "      <td>M</td>\n",
       "      <td>excellent stuff bennett photoshop would defini...</td>\n",
       "      <td>[excellent, stuff, bennett, photoshop, would, ...</td>\n",
       "      <td>[excel, stuff, bennett, photoshop, would, defi...</td>\n",
       "      <td>[excellent, stuff, bennett, photoshop, would, ...</td>\n",
       "      <td>[(excellent, JJ), (stuff, NN), (bennett, NN), ...</td>\n",
       "      <td>excellent stuff bennett photoshop would defini...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text class  \\\n",
       "114085         Lava lamps are awesome Even more so the...     M   \n",
       "149143             Poet-seers who have achieved the hi...     M   \n",
       "196033             You're so gay.  (filled with love.)...     F   \n",
       "70772                    THERE'S SOMETHING DEEPLY SURR...     M   \n",
       "156066         Have you ever had one of those days whe...     F   \n",
       "181932         omg today i .... am gonna do everything...     F   \n",
       "30928                    so today was one of the best ...     F   \n",
       "47022                  hi dideley ho neighbour!               M   \n",
       "23080          Pursuant to the last post,  urlLink Jus...     F   \n",
       "50642                As I stare into the depths of spa...     M   \n",
       "157153             Me and chandru when he come to bang...     M   \n",
       "106654             I know Mel Dobberteen from high sch...     M   \n",
       "182925         Well, some of the heavy hitters were up...     M   \n",
       "182661               (Click the link above.)  Its so s...     M   \n",
       "150595             Excellent stuff, Bennett. Photoshop...     M   \n",
       "\n",
       "                                               text_clean  \\\n",
       "114085  lava lamps awesome even possum made wax oil u ...   \n",
       "149143  poetseers achieved highest realisations endeav...   \n",
       "196033                              youre gay filled love   \n",
       "70772   theres something deeply surreal introducing bl...   \n",
       "156066  ever one days want throttle one coworkers one ...   \n",
       "181932  omg today gonna everything dont wanna woohoo i...   \n",
       "30928   today one best workouts ive well long time tha...   \n",
       "47022                             hi dideley ho neighbour   \n",
       "23080   pursuant last post urllink desserts cream liqu...   \n",
       "50642   stare depths space worry stricken nook thought...   \n",
       "157153                         chandru come banglore june   \n",
       "106654  know mel dobberteen high school mel bet bucks ...   \n",
       "182925  well heavy hitters last night demcon al gore s...   \n",
       "182661  click link stupid isnt funny well maybe little...   \n",
       "150595  excellent stuff bennett photoshop would defini...   \n",
       "\n",
       "                                              text_tokens  \\\n",
       "114085  [lava, lamps, awesome, even, possum, made, wax...   \n",
       "149143  [poetseers, achieved, highest, realisations, e...   \n",
       "196033                         [youre, gay, filled, love]   \n",
       "70772   [theres, something, deeply, surreal, introduci...   \n",
       "156066  [ever, one, days, want, throttle, one, coworke...   \n",
       "181932  [omg, today, gon, na, everything, dont, wan, n...   \n",
       "30928   [today, one, best, workouts, ive, well, long, ...   \n",
       "47022                        [hi, dideley, ho, neighbour]   \n",
       "23080   [pursuant, last, post, urllink, desserts, crea...   \n",
       "50642   [stare, depths, space, worry, stricken, nook, ...   \n",
       "157153                    [chandru, come, banglore, june]   \n",
       "106654  [know, mel, dobberteen, high, school, mel, bet...   \n",
       "182925  [well, heavy, hitters, last, night, demcon, al...   \n",
       "182661  [click, link, stupid, isnt, funny, well, maybe...   \n",
       "150595  [excellent, stuff, bennett, photoshop, would, ...   \n",
       "\n",
       "                                         text_tokens_stem  \\\n",
       "114085  [lava, lamp, awesom, even, possum, made, wax, ...   \n",
       "149143  [poetseer, achiev, highest, realis, endeavour,...   \n",
       "196033                            [your, gay, fill, love]   \n",
       "70772   [there, someth, deepli, surreal, introduc, blo...   \n",
       "156066  [ever, one, day, want, throttl, one, cowork, o...   \n",
       "181932  [omg, today, gon, na, everyth, dont, wan, na, ...   \n",
       "30928   [today, one, best, workout, ive, well, long, t...   \n",
       "47022                        [hi, dideley, ho, neighbour]   \n",
       "23080   [pursuant, last, post, urllink, dessert, cream...   \n",
       "50642   [stare, depth, space, worri, stricken, nook, t...   \n",
       "157153                     [chandru, come, banglor, june]   \n",
       "106654  [know, mel, dobberteen, high, school, mel, bet...   \n",
       "182925  [well, heavi, hitter, last, night, demcon, al,...   \n",
       "182661  [click, link, stupid, isnt, funni, well, mayb,...   \n",
       "150595  [excel, stuff, bennett, photoshop, would, defi...   \n",
       "\n",
       "                                        text_tokens_lemma  \\\n",
       "114085  [lava, lamp, awesome, even, possum, made, wax,...   \n",
       "149143  [poetseers, achieved, highest, realisation, en...   \n",
       "196033                         [youre, gay, filled, love]   \n",
       "70772   [there, something, deeply, surreal, introducin...   \n",
       "156066  [ever, one, day, want, throttle, one, coworker...   \n",
       "181932  [omg, today, gon, na, everything, dont, wan, n...   \n",
       "30928   [today, one, best, workout, ive, well, long, t...   \n",
       "47022                        [hi, dideley, ho, neighbour]   \n",
       "23080   [pursuant, last, post, urllink, dessert, cream...   \n",
       "50642   [stare, depth, space, worry, stricken, nook, t...   \n",
       "157153                    [chandru, come, banglore, june]   \n",
       "106654  [know, mel, dobberteen, high, school, mel, bet...   \n",
       "182925  [well, heavy, hitter, last, night, demcon, al,...   \n",
       "182661  [click, link, stupid, isnt, funny, well, maybe...   \n",
       "150595  [excellent, stuff, bennett, photoshop, would, ...   \n",
       "\n",
       "                                   text_tokens_pos_tagged  \\\n",
       "114085  [(lava, NN), (lamps, VBZ), (awesome, JJ), (eve...   \n",
       "149143  [(poetseers, NNS), (achieved, VBD), (highest, ...   \n",
       "196033  [(youre, NN), (gay, NN), (filled, VBN), (love,...   \n",
       "70772   [(theres, NNS), (something, NN), (deeply, RB),...   \n",
       "156066  [(ever, RB), (one, CD), (days, NNS), (want, VB...   \n",
       "181932  [(omg, NN), (today, NN), (gon, VB), (na, TO), ...   \n",
       "30928   [(today, NN), (one, CD), (best, JJS), (workout...   \n",
       "47022   [(hi, NN), (dideley, NN), (ho, VBD), (neighbou...   \n",
       "23080   [(pursuant, NN), (last, JJ), (post, NN), (urll...   \n",
       "50642   [(stare, JJ), (depths, NNS), (space, NN), (wor...   \n",
       "157153  [(chandru, NN), (come, VBN), (banglore, IN), (...   \n",
       "106654  [(know, VB), (mel, NN), (dobberteen, JJ), (hig...   \n",
       "182925  [(well, RB), (heavy, JJ), (hitters, NNS), (las...   \n",
       "182661  [(click, NN), (link, NN), (stupid, JJ), (isnt,...   \n",
       "150595  [(excellent, JJ), (stuff, NN), (bennett, NN), ...   \n",
       "\n",
       "                                                     fine  clean_length  \n",
       "114085  lava lamp awesome even possum made leave long ...           212  \n",
       "149143  poetseers achieved highest realisation endeavo...           332  \n",
       "196033                                  youre filled love            17  \n",
       "70772   there something deeply surreal introducing blo...           480  \n",
       "156066  ever want throttle coworkers today went work g...           323  \n",
       "181932  today everything dont woohoo damn excited last...           328  \n",
       "30928   today best workout well long time thats good m...           145  \n",
       "47022                                   dideley neighbour            17  \n",
       "23080   pursuant last post urllink dessert cream lique...           152  \n",
       "50642   stare depth space worry stricken nook thought ...           756  \n",
       "157153                         chandru come banglore june            26  \n",
       "106654  know dobberteen high school buck head first ma...          1041  \n",
       "182925  well heavy hitter last night demcon gore start...          2498  \n",
       "182661  click link stupid isnt funny well maybe little...          1059  \n",
       "150595  excellent stuff bennett photoshop would defini...           200  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove short words less than 3\n",
    "data_clean['fine'] = data_clean['text_tokens_lemma'].apply(lambda x: ' '.join([w for w in x if len(w)>3]))\n",
    "# Count the length of characters\n",
    "data_clean['clean_length'] = data_clean['fine'].apply(len)\n",
    "# Remove rows where character length <= 5\n",
    "data_clean = data_clean[data_clean.clean_length > 5]\n",
    "data_clean.sample(n=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197773, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(text):\n",
    "    #clean text\n",
    "    newText = text.str.lower()\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    #stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    newText = newText.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #stemmer\n",
    "    newText = newText.apply(lambda x: word_tokenize(x))\n",
    "    newText_stem = newText.apply(lambda x: word_stemmer(x))\n",
    "    #token lemma\n",
    "    newText_lem = newText.apply(lambda x: word_lemmatizer(x))\n",
    "    newText_tag = newText.apply(lambda x: word_pos_tagger(x))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare X and Y data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pub.towardsai.net/does-a-machine-know-your-gender-based-on-your-tweets-43b14740fd54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaa</th>\n",
       "      <th>aaaaaaaa</th>\n",
       "      <th>aaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaa</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzas</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197771</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197772</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197773 rows × 541317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaaa  aaaaa  aaaaaa  aaaaaaa  aaaaaaaa  aaaaaaaaa  aaaaaaaaaa  \\\n",
       "0          0      0       0        0         0          0           0   \n",
       "1          0      0       0        0         0          0           0   \n",
       "2          0      0       0        0         0          0           0   \n",
       "3          0      0       0        0         0          0           0   \n",
       "4          0      0       0        0         0          0           0   \n",
       "...      ...    ...     ...      ...       ...        ...         ...   \n",
       "197768     0      0       0        0         0          0           0   \n",
       "197769     0      0       0        0         0          0           0   \n",
       "197770     0      0       0        0         0          0           0   \n",
       "197771     0      0       0        0         0          0           0   \n",
       "197772     0      0       0        0         0          0           0   \n",
       "\n",
       "        aaaaaaaaaaa  aaaaaaaaaaaa  aaaaaaaaaaaaa  ...  \\\n",
       "0                 0             0              0  ...   \n",
       "1                 0             0              0  ...   \n",
       "2                 0             0              0  ...   \n",
       "3                 0             0              0  ...   \n",
       "4                 0             0              0  ...   \n",
       "...             ...           ...            ...  ...   \n",
       "197768            0             0              0  ...   \n",
       "197769            0             0              0  ...   \n",
       "197770            0             0              0  ...   \n",
       "197771            0             0              0  ...   \n",
       "197772            0             0              0  ...   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "...                                   ...   \n",
       "197768                                  0   \n",
       "197769                                  0   \n",
       "197770                                  0   \n",
       "197771                                  0   \n",
       "197772                                  0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzas  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "...                                     ...   \n",
       "197768                                    0   \n",
       "197769                                    0   \n",
       "197770                                    0   \n",
       "197771                                    0   \n",
       "197772                                    0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "...                                        ...   \n",
       "197768                                       0   \n",
       "197769                                       0   \n",
       "197770                                       0   \n",
       "197771                                       0   \n",
       "197772                                       0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                     0   \n",
       "1                                                     0   \n",
       "2                                                     0   \n",
       "3                                                     0   \n",
       "4                                                     0   \n",
       "...                                                 ...   \n",
       "197768                                                0   \n",
       "197769                                                0   \n",
       "197770                                                0   \n",
       "197771                                                0   \n",
       "197772                                                0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                      0   \n",
       "1                                                      0   \n",
       "2                                                      0   \n",
       "3                                                      0   \n",
       "4                                                      0   \n",
       "...                                                  ...   \n",
       "197768                                                 0   \n",
       "197769                                                 0   \n",
       "197770                                                 0   \n",
       "197771                                                 0   \n",
       "197772                                                 0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                       0         \n",
       "1                                                       0         \n",
       "2                                                       0         \n",
       "3                                                       0         \n",
       "4                                                       0         \n",
       "...                                                   ...         \n",
       "197768                                                  0         \n",
       "197769                                                  0         \n",
       "197770                                                  0         \n",
       "197771                                                  0         \n",
       "197772                                                  0         \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                       0            \n",
       "1                                                       0            \n",
       "2                                                       0            \n",
       "3                                                       0            \n",
       "4                                                       0            \n",
       "...                                                   ...            \n",
       "197768                                                  0            \n",
       "197769                                                  0            \n",
       "197770                                                  0            \n",
       "197771                                                  0            \n",
       "197772                                                  0            \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                       0              \n",
       "1                                                       0              \n",
       "2                                                       0              \n",
       "3                                                       0              \n",
       "4                                                       0              \n",
       "...                                                   ...              \n",
       "197768                                                  0              \n",
       "197769                                                  0              \n",
       "197770                                                  0              \n",
       "197771                                                  0              \n",
       "197772                                                  0              \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                                       0                   \n",
       "1                                                       0                   \n",
       "2                                                       0                   \n",
       "3                                                       0                   \n",
       "4                                                       0                   \n",
       "...                                                   ...                   \n",
       "197768                                                  0                   \n",
       "197769                                                  0                   \n",
       "197770                                                  0                   \n",
       "197771                                                  0                   \n",
       "197772                                                  0                   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                                       0                                            \n",
       "1                                                       0                                            \n",
       "2                                                       0                                            \n",
       "3                                                       0                                            \n",
       "4                                                       0                                            \n",
       "...                                                   ...                                            \n",
       "197768                                                  0                                            \n",
       "197769                                                  0                                            \n",
       "197770                                                  0                                            \n",
       "197771                                                  0                                            \n",
       "197772                                                  0                                            \n",
       "\n",
       "[197773 rows x 541317 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import f1_score\n",
    "# Bag-of-words features\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(data_clean['fine'])\n",
    "df_bow = pd.DataFrame(bow.todense(), columns=bow_vectorizer.get_feature_names())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['gender'] = data_clean['class'].apply(lambda x: 1 if x=='F' else 0)\n",
    "# Splitting the data into training and test set\n",
    "X = df_bow\n",
    "y = data_clean['gender']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting on Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "prediction_bow = logreg.predict_proba(X_test)\n",
    "# Calculating the F1 score\n",
    "# If prediction is greater than or equal to 0.5 than 1, else 0\n",
    "# Gender, 0 = male and 1 = female\n",
    "prediction_int = prediction_bow[:,1]>=0.5\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "# Calculating F1 score\n",
    "log_bow = f1_score(y_test, prediction_int)\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB()),\n",
    " #                      ])\n",
    "#text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = text_clf.predict(X_test)\n",
    "#np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter_no_change=5, random_state=42)\n",
    "_ = clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = text_df[\"text\"]\n",
    "y1 = text_df[\"class\"].apply(lambda x: 1 if x=='F' else 0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    " ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train2, y_train2)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test2)\n",
    "np.mean(predicted_mnb_stemmed == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Neuron Network: https://realpython.com/python-keras-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
