{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification from Text Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!  conda install  openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_excel('blog-gender-dataset/blog-gender-dataset.xlsx', engine='openpyxl')\n",
    "headers = [\"text\", \"class\"]\n",
    "text_df = pd.read_csv('blog-gender-dataset/blog-gender-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3231 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.iloc[:, [0, 1]]\n",
    "text_df.columns = headers\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3231, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3226\n",
       "unique       8\n",
       "top          M\n",
       "freq      1546\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"class\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3231 entries, 0 to 3230\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3224 non-null   object\n",
      " 1   class   3226 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 50.6+ KB\n"
     ]
    }
   ],
   "source": [
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1546, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"M\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"F\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3224 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3224 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.dropna()\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \" \".join([c for c in text if c not in punctuation])\n",
    "    return no_punct\n",
    "#textclean = text_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "#instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#textclean = textclean.apply(lambda x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>who moved my cheese   the world has been devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday i attended a biweekly meeting of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam is nothing like natalie natalie never we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>in the eu we have browser choice but few know ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \n",
       "0   guest demo eric iversons itty bitty searchfeb...  \n",
       "1  who moved my cheese   the world has been devel...  \n",
       "2   yesterday i attended a biweekly meeting of an...  \n",
       "3   liam is nothing like natalie natalie never we...  \n",
       "4  in the eu we have browser choice but few know ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n",
    "data_clean = clean_text(text_df, 'text', 'text_clean')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \n",
       "0  guest demo eric iversons itty bitty searchfebr...  \n",
       "1  moved cheese world developing areas create dif...  \n",
       "2  yesterday attended biweekly meeting informal u...  \n",
       "3  liam nothing like natalie natalie never went d...  \n",
       "4  eu browser choice know eu tougher monopolies a...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk.corpus\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guest demo eric iversons itty bitty searchfebruary th daniel tunkelangrespondim back vacation still digging way everything thats piled ive offlinewhile catch thought id share demo eric iverson gracious enough share uses yahoo boss support exploratory search experience top general web search enginewhen perform query application retrieves set related term candidates using yahoos key terms api scores term dividing occurrence count within result set global occurrence counta relevance measure similar one former colleagues used endeca enterprise contextsyou try demo rough edges produces nice resultsespecially considering simplicity approachheres example used application explore learn something new started information retrieval noticed interactive information retrieval top term used refine refinement suggestions looked familiar mebut unfamiliar name caught attention anton leuski following curiosity refined looking results immediately saw leuski done work evaluating document clustering interactive information retrieval exploration made clear someone whose work get knowcheck home pagei cant promise youll productive experience encourage try erics demo simple examples like remind value pursuing hcir open webspeaking hcir works well flesh details next weeks course ill share'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['text_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...  \n",
       "1  [move, chees, world, develop, area, creat, dif...  \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...  \n",
       "3  [liam, noth, like, natali, natali, never, went...  \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk \n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "      <td>[moved, cheese, world, developing, area, creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...   \n",
       "1  [move, chees, world, develop, area, creat, dif...   \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...   \n",
       "3  [liam, noth, like, natali, natali, never, went...   \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...   \n",
       "\n",
       "                                   text_tokens_lemma  \n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...  \n",
       "1  [moved, cheese, world, developing, area, creat...  \n",
       "2  [yesterday, attended, biweekly, meeting, infor...  \n",
       "3  [liam, nothing, like, natalie, natalie, never,...  \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[(guest, JJS), (demo, NN), (eric, JJ), (iverso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "      <td>[moved, cheese, world, developing, area, creat...</td>\n",
       "      <td>[(moved, VBN), (cheese, JJ), (world, NN), (dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[(yesterday, NN), (attended, VBD), (biweekly, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[(liam, RB), (nothing, NN), (like, IN), (natal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[(eu, RB), (browser, NN), (choice, NN), (know,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...   \n",
       "1  [move, chees, world, develop, area, creat, dif...   \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...   \n",
       "3  [liam, noth, like, natali, natali, never, went...   \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...   \n",
       "\n",
       "                                   text_tokens_lemma  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, area, creat...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                              text_tokens_pos_tagged  \n",
       "0  [(guest, JJS), (demo, NN), (eric, JJ), (iverso...  \n",
       "1  [(moved, VBN), (cheese, JJ), (world, NN), (dev...  \n",
       "2  [(yesterday, NN), (attended, VBD), (biweekly, ...  \n",
       "3  [(liam, RB), (nothing, NN), (like, IN), (natal...  \n",
       "4  [(eu, RB), (browser, NN), (choice, NN), (know,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of speech tagging and chunking\n",
    "def word_pos_tagger(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    return pos_tagged_text\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_clean['text_tokens_pos_tagged'] = data_clean['text_tokens'].apply(lambda x: word_pos_tagger(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "      <th>fine</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Others might have thought I’ve gone bonkers bu...</td>\n",
       "      <td>F</td>\n",
       "      <td>others might thought ive gone bonkers somethin...</td>\n",
       "      <td>[others, might, thought, ive, gone, bonkers, s...</td>\n",
       "      <td>[other, might, thought, ive, gone, bonker, som...</td>\n",
       "      <td>[others, might, thought, ive, gone, bonkers, s...</td>\n",
       "      <td>[(others, NNS), (might, MD), (thought, VB), (i...</td>\n",
       "      <td>others might thought gone bonkers something al...</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>The World Health Organization said the Swine F...</td>\n",
       "      <td>F</td>\n",
       "      <td>world health organization said swine flu appea...</td>\n",
       "      <td>[world, health, organization, said, swine, flu...</td>\n",
       "      <td>[world, health, organ, said, swine, flu, appea...</td>\n",
       "      <td>[world, health, organization, said, swine, flu...</td>\n",
       "      <td>[(world, NN), (health, NN), (organization, NN)...</td>\n",
       "      <td>world health organization said swine appear sp...</td>\n",
       "      <td>4430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>I've just got back from the hairdresser's, and...</td>\n",
       "      <td>F</td>\n",
       "      <td>ive got back hairdressers talking left got bac...</td>\n",
       "      <td>[ive, got, back, hairdressers, talking, left, ...</td>\n",
       "      <td>[ive, got, back, hairdress, talk, left, got, b...</td>\n",
       "      <td>[ive, got, back, hairdresser, talking, left, g...</td>\n",
       "      <td>[(ive, JJ), (got, VBD), (back, RB), (hairdress...</td>\n",
       "      <td>back hairdresser talking left back dont know w...</td>\n",
       "      <td>5556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Suvendu Roy, of Titan Industries shares his in...</td>\n",
       "      <td>M</td>\n",
       "      <td>suvendu roy titan industries shares inspiratio...</td>\n",
       "      <td>[suvendu, roy, titan, industries, shares, insp...</td>\n",
       "      <td>[suvendu, roy, titan, industri, share, inspir,...</td>\n",
       "      <td>[suvendu, roy, titan, industry, share, inspira...</td>\n",
       "      <td>[(suvendu, NN), (roy, NN), (titan, NN), (indus...</td>\n",
       "      <td>suvendu titan industry share inspirational enc...</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Excess - There is an old saying in sanskrit, \"...</td>\n",
       "      <td>M</td>\n",
       "      <td>excess old saying sanskrit ati sarwagya warjya...</td>\n",
       "      <td>[excess, old, saying, sanskrit, ati, sarwagya,...</td>\n",
       "      <td>[excess, old, say, sanskrit, ati, sarwagya, wa...</td>\n",
       "      <td>[excess, old, saying, sanskrit, ati, sarwagya,...</td>\n",
       "      <td>[(excess, JJ), (old, JJ), (saying, VBG), (sans...</td>\n",
       "      <td>excess saying sanskrit sarwagya warjyate mean ...</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>I certainly hope AIPAC comes to its senses soo...</td>\n",
       "      <td>M</td>\n",
       "      <td>certainly hope aipac comes senses soon suspect...</td>\n",
       "      <td>[certainly, hope, aipac, comes, senses, soon, ...</td>\n",
       "      <td>[certainli, hope, aipac, come, sens, soon, sus...</td>\n",
       "      <td>[certainly, hope, aipac, come, sens, soon, sus...</td>\n",
       "      <td>[(certainly, RB), (hope, VBP), (aipac, NNS), (...</td>\n",
       "      <td>certainly hope aipac come sens soon suspect wa...</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>Cell phones have allowed us a tremendous amoun...</td>\n",
       "      <td>M</td>\n",
       "      <td>cell phones allowed us tremendous amount freed...</td>\n",
       "      <td>[cell, phones, allowed, us, tremendous, amount...</td>\n",
       "      <td>[cell, phone, allow, us, tremend, amount, free...</td>\n",
       "      <td>[cell, phone, allowed, u, tremendous, amount, ...</td>\n",
       "      <td>[(cell, NN), (phones, NNS), (allowed, VBD), (u...</td>\n",
       "      <td>cell phone allowed tremendous amount freedom t...</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>I love this show sooo much. its very interesti...</td>\n",
       "      <td>M</td>\n",
       "      <td>love show sooo much interesting charecters rea...</td>\n",
       "      <td>[love, show, sooo, much, interesting, charecte...</td>\n",
       "      <td>[love, show, sooo, much, interest, charect, re...</td>\n",
       "      <td>[love, show, sooo, much, interesting, charecte...</td>\n",
       "      <td>[(love, VB), (show, NN), (sooo, RB), (much, JJ...</td>\n",
       "      <td>love show sooo much interesting charecters rea...</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>ust a quick LOST-related thought. I think that...</td>\n",
       "      <td>M</td>\n",
       "      <td>ust quick lostrelated thought think discussion...</td>\n",
       "      <td>[ust, quick, lostrelated, thought, think, disc...</td>\n",
       "      <td>[ust, quick, lostrel, thought, think, discuss,...</td>\n",
       "      <td>[ust, quick, lostrelated, thought, think, disc...</td>\n",
       "      <td>[(ust, JJ), (quick, NN), (lostrelated, VBD), (...</td>\n",
       "      <td>quick lostrelated thought think discussion sea...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Back. Bigger. Better. Ok so I achieved 1 out ...</td>\n",
       "      <td>F</td>\n",
       "      <td>back bigger better ok achieved promised bs im ...</td>\n",
       "      <td>[back, bigger, better, ok, achieved, promised,...</td>\n",
       "      <td>[back, bigger, better, ok, achiev, promis, bs,...</td>\n",
       "      <td>[back, bigger, better, ok, achieved, promised,...</td>\n",
       "      <td>[(back, RB), (bigger, JJR), (better, RBR), (ok...</td>\n",
       "      <td>back bigger better achieved promised back bigg...</td>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>At the Game Developers Conference, which just ...</td>\n",
       "      <td>F</td>\n",
       "      <td>game developers conference wrapped yesterday m...</td>\n",
       "      <td>[game, developers, conference, wrapped, yester...</td>\n",
       "      <td>[game, develop, confer, wrap, yesterday, mani,...</td>\n",
       "      <td>[game, developer, conference, wrapped, yesterd...</td>\n",
       "      <td>[(game, NN), (developers, NNS), (conference, N...</td>\n",
       "      <td>game developer conference wrapped yesterday ma...</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Sure, it is a new world out there! Especially...</td>\n",
       "      <td>M</td>\n",
       "      <td>sure new world especially owned yahoo stock la...</td>\n",
       "      <td>[sure, new, world, especially, owned, yahoo, s...</td>\n",
       "      <td>[sure, new, world, especi, own, yahoo, stock, ...</td>\n",
       "      <td>[sure, new, world, especially, owned, yahoo, s...</td>\n",
       "      <td>[(sure, JJ), (new, JJ), (world, NN), (especial...</td>\n",
       "      <td>sure world especially owned yahoo stock last n...</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>I'm the proud Mommy of Wesley(8/2005) and Ryan...</td>\n",
       "      <td>F</td>\n",
       "      <td>im proud mommy wesley ryan married amazing man...</td>\n",
       "      <td>[im, proud, mommy, wesley, ryan, married, amaz...</td>\n",
       "      <td>[im, proud, mommi, wesley, ryan, marri, amaz, ...</td>\n",
       "      <td>[im, proud, mommy, wesley, ryan, married, amaz...</td>\n",
       "      <td>[(im, JJ), (proud, JJ), (mommy, NN), (wesley, ...</td>\n",
       "      <td>proud mommy wesley ryan married amazing childr...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>I think the thing that is really interesting a...</td>\n",
       "      <td>M</td>\n",
       "      <td>think thing really interesting old radio progr...</td>\n",
       "      <td>[think, thing, really, interesting, old, radio...</td>\n",
       "      <td>[think, thing, realli, interest, old, radio, p...</td>\n",
       "      <td>[think, thing, really, interesting, old, radio...</td>\n",
       "      <td>[(think, VB), (thing, NN), (really, RB), (inte...</td>\n",
       "      <td>think thing really interesting radio program i...</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>Doing good so far on the 3-day weekend fast. N...</td>\n",
       "      <td>F</td>\n",
       "      <td>good far day weekend fast nothing water yester...</td>\n",
       "      <td>[good, far, day, weekend, fast, nothing, water...</td>\n",
       "      <td>[good, far, day, weekend, fast, noth, water, y...</td>\n",
       "      <td>[good, far, day, weekend, fast, nothing, water...</td>\n",
       "      <td>[(good, JJ), (far, RB), (day, NN), (weekend, N...</td>\n",
       "      <td>good weekend fast nothing water yesterday morn...</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class  \\\n",
       "825   Others might have thought I’ve gone bonkers bu...     F   \n",
       "2890  The World Health Organization said the Swine F...     F   \n",
       "2780  I've just got back from the hairdresser's, and...     F   \n",
       "385   Suvendu Roy, of Titan Industries shares his in...     M   \n",
       "173   Excess - There is an old saying in sanskrit, \"...     M   \n",
       "829   I certainly hope AIPAC comes to its senses soo...     M   \n",
       "1828  Cell phones have allowed us a tremendous amoun...     M   \n",
       "1597  I love this show sooo much. its very interesti...     M   \n",
       "1558  ust a quick LOST-related thought. I think that...     M   \n",
       "1280   Back. Bigger. Better. Ok so I achieved 1 out ...     F   \n",
       "1793  At the Game Developers Conference, which just ...     F   \n",
       "134    Sure, it is a new world out there! Especially...     M   \n",
       "1733  I'm the proud Mommy of Wesley(8/2005) and Ryan...     F   \n",
       "1752  I think the thing that is really interesting a...     M   \n",
       "687   Doing good so far on the 3-day weekend fast. N...     F   \n",
       "\n",
       "                                             text_clean  \\\n",
       "825   others might thought ive gone bonkers somethin...   \n",
       "2890  world health organization said swine flu appea...   \n",
       "2780  ive got back hairdressers talking left got bac...   \n",
       "385   suvendu roy titan industries shares inspiratio...   \n",
       "173   excess old saying sanskrit ati sarwagya warjya...   \n",
       "829   certainly hope aipac comes senses soon suspect...   \n",
       "1828  cell phones allowed us tremendous amount freed...   \n",
       "1597  love show sooo much interesting charecters rea...   \n",
       "1558  ust quick lostrelated thought think discussion...   \n",
       "1280  back bigger better ok achieved promised bs im ...   \n",
       "1793  game developers conference wrapped yesterday m...   \n",
       "134   sure new world especially owned yahoo stock la...   \n",
       "1733  im proud mommy wesley ryan married amazing man...   \n",
       "1752  think thing really interesting old radio progr...   \n",
       "687   good far day weekend fast nothing water yester...   \n",
       "\n",
       "                                            text_tokens  \\\n",
       "825   [others, might, thought, ive, gone, bonkers, s...   \n",
       "2890  [world, health, organization, said, swine, flu...   \n",
       "2780  [ive, got, back, hairdressers, talking, left, ...   \n",
       "385   [suvendu, roy, titan, industries, shares, insp...   \n",
       "173   [excess, old, saying, sanskrit, ati, sarwagya,...   \n",
       "829   [certainly, hope, aipac, comes, senses, soon, ...   \n",
       "1828  [cell, phones, allowed, us, tremendous, amount...   \n",
       "1597  [love, show, sooo, much, interesting, charecte...   \n",
       "1558  [ust, quick, lostrelated, thought, think, disc...   \n",
       "1280  [back, bigger, better, ok, achieved, promised,...   \n",
       "1793  [game, developers, conference, wrapped, yester...   \n",
       "134   [sure, new, world, especially, owned, yahoo, s...   \n",
       "1733  [im, proud, mommy, wesley, ryan, married, amaz...   \n",
       "1752  [think, thing, really, interesting, old, radio...   \n",
       "687   [good, far, day, weekend, fast, nothing, water...   \n",
       "\n",
       "                                       text_tokens_stem  \\\n",
       "825   [other, might, thought, ive, gone, bonker, som...   \n",
       "2890  [world, health, organ, said, swine, flu, appea...   \n",
       "2780  [ive, got, back, hairdress, talk, left, got, b...   \n",
       "385   [suvendu, roy, titan, industri, share, inspir,...   \n",
       "173   [excess, old, say, sanskrit, ati, sarwagya, wa...   \n",
       "829   [certainli, hope, aipac, come, sens, soon, sus...   \n",
       "1828  [cell, phone, allow, us, tremend, amount, free...   \n",
       "1597  [love, show, sooo, much, interest, charect, re...   \n",
       "1558  [ust, quick, lostrel, thought, think, discuss,...   \n",
       "1280  [back, bigger, better, ok, achiev, promis, bs,...   \n",
       "1793  [game, develop, confer, wrap, yesterday, mani,...   \n",
       "134   [sure, new, world, especi, own, yahoo, stock, ...   \n",
       "1733  [im, proud, mommi, wesley, ryan, marri, amaz, ...   \n",
       "1752  [think, thing, realli, interest, old, radio, p...   \n",
       "687   [good, far, day, weekend, fast, noth, water, y...   \n",
       "\n",
       "                                      text_tokens_lemma  \\\n",
       "825   [others, might, thought, ive, gone, bonkers, s...   \n",
       "2890  [world, health, organization, said, swine, flu...   \n",
       "2780  [ive, got, back, hairdresser, talking, left, g...   \n",
       "385   [suvendu, roy, titan, industry, share, inspira...   \n",
       "173   [excess, old, saying, sanskrit, ati, sarwagya,...   \n",
       "829   [certainly, hope, aipac, come, sens, soon, sus...   \n",
       "1828  [cell, phone, allowed, u, tremendous, amount, ...   \n",
       "1597  [love, show, sooo, much, interesting, charecte...   \n",
       "1558  [ust, quick, lostrelated, thought, think, disc...   \n",
       "1280  [back, bigger, better, ok, achieved, promised,...   \n",
       "1793  [game, developer, conference, wrapped, yesterd...   \n",
       "134   [sure, new, world, especially, owned, yahoo, s...   \n",
       "1733  [im, proud, mommy, wesley, ryan, married, amaz...   \n",
       "1752  [think, thing, really, interesting, old, radio...   \n",
       "687   [good, far, day, weekend, fast, nothing, water...   \n",
       "\n",
       "                                 text_tokens_pos_tagged  \\\n",
       "825   [(others, NNS), (might, MD), (thought, VB), (i...   \n",
       "2890  [(world, NN), (health, NN), (organization, NN)...   \n",
       "2780  [(ive, JJ), (got, VBD), (back, RB), (hairdress...   \n",
       "385   [(suvendu, NN), (roy, NN), (titan, NN), (indus...   \n",
       "173   [(excess, JJ), (old, JJ), (saying, VBG), (sans...   \n",
       "829   [(certainly, RB), (hope, VBP), (aipac, NNS), (...   \n",
       "1828  [(cell, NN), (phones, NNS), (allowed, VBD), (u...   \n",
       "1597  [(love, VB), (show, NN), (sooo, RB), (much, JJ...   \n",
       "1558  [(ust, JJ), (quick, NN), (lostrelated, VBD), (...   \n",
       "1280  [(back, RB), (bigger, JJR), (better, RBR), (ok...   \n",
       "1793  [(game, NN), (developers, NNS), (conference, N...   \n",
       "134   [(sure, JJ), (new, JJ), (world, NN), (especial...   \n",
       "1733  [(im, JJ), (proud, JJ), (mommy, NN), (wesley, ...   \n",
       "1752  [(think, VB), (thing, NN), (really, RB), (inte...   \n",
       "687   [(good, JJ), (far, RB), (day, NN), (weekend, N...   \n",
       "\n",
       "                                                   fine  clean_length  \n",
       "825   others might thought gone bonkers something al...           365  \n",
       "2890  world health organization said swine appear sp...          4430  \n",
       "2780  back hairdresser talking left back dont know w...          5556  \n",
       "385   suvendu titan industry share inspirational enc...          1062  \n",
       "173   excess saying sanskrit sarwagya warjyate mean ...           793  \n",
       "829   certainly hope aipac come sens soon suspect wa...           445  \n",
       "1828  cell phone allowed tremendous amount freedom t...           729  \n",
       "1597  love show sooo much interesting charecters rea...           275  \n",
       "1558  quick lostrelated thought think discussion sea...           247  \n",
       "1280  back bigger better achieved promised back bigg...          3586  \n",
       "1793  game developer conference wrapped yesterday ma...           306  \n",
       "134   sure world especially owned yahoo stock last n...           614  \n",
       "1733  proud mommy wesley ryan married amazing childr...            95  \n",
       "1752  think thing really interesting radio program i...           335  \n",
       "687   good weekend fast nothing water yesterday morn...           909  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove short words less than 3\n",
    "data_clean['fine'] = data_clean['text_tokens_lemma'].apply(lambda x: ' '.join([w for w in x if len(w)>3]))\n",
    "# Count the length of characters\n",
    "data_clean['clean_length'] = data_clean['fine'].apply(len)\n",
    "# Remove rows where character length <= 5\n",
    "data_clean = data_clean[data_clean.clean_length > 5]\n",
    "data_clean.sample(n=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3223, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(text):\n",
    "    #clean text\n",
    "    newText = text.str.lower()\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    #stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    newText = newText.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #stemmer\n",
    "    newText = newText.apply(lambda x: word_tokenize(x))\n",
    "    newText_stem = newText.apply(lambda x: word_stemmer(x))\n",
    "    #token lemma\n",
    "    newText_lem = newText.apply(lambda x: word_lemmatizer(x))\n",
    "    newText_tag = newText.apply(lambda x: word_pos_tagger(x))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare X and Y data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pub.towardsai.net/does-a-machine-know-your-gender-based-on-your-tweets-43b14740fd54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaaahs</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aacross</th>\n",
       "      <th>aada</th>\n",
       "      <th>aadi</th>\n",
       "      <th>...</th>\n",
       "      <th>zwei</th>\n",
       "      <th>zwierzynski</th>\n",
       "      <th>zydecodancing</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zyprexaand</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zzzzs</th>\n",
       "      <th>zzzzzs</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3223 rows × 58567 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaa  aaaaah  aaaaand  aaaah  aaaahs  aaaand  aaah  aacross  aada  aadi  \\\n",
       "0        0       0        0      0       0       0     0        0     0     0   \n",
       "1        0       0        0      0       0       0     0        0     0     0   \n",
       "2        0       0        0      0       0       0     0        0     0     0   \n",
       "3        0       0        0      0       0       0     0        0     0     0   \n",
       "4        0       0        0      0       0       0     0        0     0     0   \n",
       "...    ...     ...      ...    ...     ...     ...   ...      ...   ...   ...   \n",
       "3218     0       0        0      0       0       0     0        0     0     0   \n",
       "3219     0       0        0      0       0       0     0        0     0     0   \n",
       "3220     0       0        0      0       0       0     0        0     0     0   \n",
       "3221     0       0        0      0       0       0     0        0     0     0   \n",
       "3222     0       0        0      0       0       0     0        0     0     0   \n",
       "\n",
       "      ...  zwei  zwierzynski  zydecodancing  zynga  zyprexaand  zyrtec  zzzzs  \\\n",
       "0     ...     0            0              0      0           0       0      0   \n",
       "1     ...     0            0              0      0           0       0      0   \n",
       "2     ...     0            0              0      0           0       0      0   \n",
       "3     ...     0            0              0      0           0       0      0   \n",
       "4     ...     0            0              0      0           0       0      0   \n",
       "...   ...   ...          ...            ...    ...         ...     ...    ...   \n",
       "3218  ...     0            0              0      0           0       0      0   \n",
       "3219  ...     0            0              0      0           0       0      0   \n",
       "3220  ...     0            0              0      0           0       0      0   \n",
       "3221  ...     0            0              0      0           0       0      0   \n",
       "3222  ...     0            0              0      0           0       0      0   \n",
       "\n",
       "      zzzzzs  zzzzzzzzzzzzzzzzzz  \\\n",
       "0          0                   0   \n",
       "1          0                   0   \n",
       "2          0                   0   \n",
       "3          0                   0   \n",
       "4          0                   0   \n",
       "...      ...                 ...   \n",
       "3218       0                   0   \n",
       "3219       0                   0   \n",
       "3220       0                   0   \n",
       "3221       0                   0   \n",
       "3222       0                   0   \n",
       "\n",
       "      zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                                     0                        \n",
       "1                                                     0                        \n",
       "2                                                     0                        \n",
       "3                                                     0                        \n",
       "4                                                     0                        \n",
       "...                                                 ...                        \n",
       "3218                                                  0                        \n",
       "3219                                                  0                        \n",
       "3220                                                  0                        \n",
       "3221                                                  0                        \n",
       "3222                                                  0                        \n",
       "\n",
       "[3223 rows x 58567 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import f1_score\n",
    "# Bag-of-words features\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(data_clean['fine'])\n",
    "df_bow = pd.DataFrame(bow.todense(), columns=bow_vectorizer.get_feature_names())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_clean['gender'] = data_clean['class'].apply(lambda x: 1 if x=='F' else 0)\n",
    "# Splitting the data into training and test set\n",
    "X = df_bow\n",
    "y = data_clean['gender']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6431226765799257"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting on Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "prediction_bow = logreg.predict_proba(X_test)\n",
    "# Calculating the F1 score\n",
    "# If prediction is greater than or equal to 0.5 than 1, else 0\n",
    "# Gender, 0 = male and 1 = female\n",
    "prediction_int = prediction_bow[:,1]>=0.5\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "# Calculating F1 score\n",
    "log_bow = f1_score(y_test, prediction_int)\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7023255813953488\n"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280  79]\n",
      " [113 173]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6930232558139535"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB()),\n",
    " #                      ])\n",
    "#text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = text_clf.predict(X_test)\n",
    "#np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6821705426356589"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter_no_change=5, random_state=42)\n",
    "_ = clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = text_df[\"text\"]\n",
    "y1 = text_df[\"class\"].apply(lambda x: 1 if x=='F' else 0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6790697674418604"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    " ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train2, y_train2)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test2)\n",
    "np.mean(predicted_mnb_stemmed == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
