{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification from Text Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!  conda install  openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_excel('blog-gender-dataset/blog-gender-dataset.xlsx', engine='openpyxl')\n",
    "headers = [\"text\", \"class\"]\n",
    "text_df = pd.read_csv('blog-gender-dataset/blog-gender-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3231 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.iloc[:, [0, 1]]\n",
    "text_df.columns = headers\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3231, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3226\n",
       "unique       8\n",
       "top          M\n",
       "freq      1546\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"class\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3231 entries, 0 to 3230\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3224 non-null   object\n",
      " 1   class   3226 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 50.6+ KB\n"
     ]
    }
   ],
   "source": [
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1546, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"M\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['class']==\"F\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>It was a scavenger style race with checkpoints...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>Finally! I got a full day's work done. Almost ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>At the height of laughter, the universe is flu...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>I like birds, especially woodpeckers and MOST ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>Oh friends, it's finally here! I thought the m...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3224 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class\n",
       "0      Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M\n",
       "1     Who moved my Cheese???   The world has been de...     M\n",
       "2      Yesterday I attended a biweekly meeting of an...     M\n",
       "3      Liam is nothing like Natalie. Natalie never w...     F\n",
       "4     In the EU we have browser choice, but few know...     M\n",
       "...                                                 ...   ...\n",
       "3226  It was a scavenger style race with checkpoints...     M\n",
       "3227  Finally! I got a full day's work done. Almost ...     F\n",
       "3228  At the height of laughter, the universe is flu...     M\n",
       "3229  I like birds, especially woodpeckers and MOST ...     M\n",
       "3230  Oh friends, it's finally here! I thought the m...     F\n",
       "\n",
       "[3224 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.dropna()\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M    1677\n",
       "F    1547\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"class\"] = text_df[\"class\"].replace(' F', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('f', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('F ', 'F')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace(' M', 'M')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace('m', 'M')\n",
    "text_df[\"class\"] = text_df[\"class\"].replace(' M ', 'M')\n",
    "text_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who moved my Cheese???   The world has been developing in and out in all the areas and to create a difference in this competitive world... we need to change...\\nchange the way we take our things...\\nbut we rather change or atleast try to change..... we try the same routine work evryday and expect to get more\\nand when things fail such as losing a job, loss in business we would upset, discouraged, frustrated and keep on hanging to the same thing again and start complaining.\\nCHANGE IS GOOD.... LETS WELCOME IT..!!!\\nwondering wat is all about Cheese?? and what actually is all about \" Who Moved My Cheese??? \"\\n\\n\\nWell...!!!! Who moved my cheese?? is a simple parable that reveals profound thoughts. It is an enlightening story of four characters who live in a maze and look for cheese to nourish them and make them happy. The story is about two mice called \"SNIFF\" and \"SCURRY\" and two little men ... smaller in size and who were similar to us people. Their names were \"HEM\" and \"HAW\"\\n\\nCheese is a metaphor for what you want to have in life - whether it is a good job, loving relationship, money or a possession, health or spiritual peace of mind.\\n\\nAnd the maze is where you look for what you want - the organisation you work in, or the family or community you live in.\\n\\nEveryday both the mice and men spent time in the maze looking for their own special cheese. The mice had only Rodent brains while the men used their brains, filled with many beliefs. The common thing between the rodents and these men is tat every morning they went in search for the cheese.\\n\\nThe maze had confusing network of paths corridors and some of the chambers had delicious cheese. But there were also dark corners and blind alleys leading nowhere. It was an easy place to get lost. Those who found the way to secret chambers had to enjoy a better life.\\n\\nAfter much of searching they found a chamber C which had an enormous amount of richest quality of Cheese.Rodents used the trial and error method by racing through the corridors. While the men used their brains and beliefs and experience.\\n\\nHAVING CHEESE MADE THEM HAPPY.\\nBoth these men and rodents enjoyed the cheese everyday almost many many days and finally one day was such tat the cheese had to run out and there isnt left anymore.\\nRodents observed the minute details of cheese getting decreased day by day and they were ready for the situation and were ready for the hunt of new Cheese.\\nOn the otherhand these men based on their beliefs wondered \"Who moved their Cheese?\". Both HEM and HAW kept waiting for the cheese at the old place. Fearing that they might get lost in the maze they didnt start to find their way to new cheese and started starving and waiting in the old place. Both of them kept Hemming and Hawing for their old Cheese. But couldnt find any and increased frustration in them. It also created FEAR in them about what might happend if they get lost in finding new cheese..!!!\\nThey started evaluating the situation at Chamber C and waited there everyday and returned homes hungry.\\nTHEY WERE AFRAID OF CHANGE.\\n\\nMeanwhile,Rodents found out their new way and found a chamber N having more cheese than the older one.\\n\\nEverytime HAW wanted to go in search for the new Cheese. HEM would stop him. But one day in utter despair to find the new Cheese HAW started for his search leaving behind his friend HEM in the old chamber. He wrote on a wall so tat his friend had something to think about...\\nIF YOU DONOT CHANGE, YOU CAN BECOME EXTINCT.\\nand HAW in order to overcome his fear of getting lost in the maze ... he reminded himself as..\\nWHAT WOULD YOU DO IF YOU WEREN\\'T AFRAID?\\nhe continued his search and found a dark alley where he could smell some nice cheese\\nhe was afraid of what might be on the other end... and started walking towards it.\\nAND when he moved he felt free and rather more young than he is.\\n\\nWHEN YOU MOVE BEYOND YOUR FEAR, YOU FEEL FREE.!!!\\nbut he didnt find any cheese it had some little pieces left over near the chamber, someone had already visited the chamber and completed it. He gulped some cheese leftover and kept some for his friend in his pocket and wondered if HEM was still in chamber C\\n\\nAll the while throughtout his search he kept on noticing the changes in him.. and everytime he is feeling happy and on the way back to his friend HEM , HAW had been thinking of himself as..\\nwhy he hadnt moved out before to find the new cheese...??\\nTHE QUICKER YOU LET GO , OF OLD CHEESE THE SOONER YOU FIND NEW CHEESE.\\nIT IS SAFER TO SEARCH IN THE MAZE THAN REMAIN IN A CHEESELESS SITUATION.\\nOLD BELIEFS DONOT LEAD YOU TO NEW CHEESE.\\nHAW came back to his friend HEW and told his experience expecting him to turn out for searching. but HEW didnt.\\nso he left him there and returned for his new treasure. before coming he told him about the way he felt and shared the cheese wid him.\\nHAW started his journey back to the maze.\\n\\nWHEN YOU SEE THAT YOU CAN FIND AND ENJOY NEW CHEESE, YOU CHANGE COURSE.\\nNOTICING SMALL CHANGES EARLY HELPS YOU ADAPT TO BIGGER CHANGES THAT ARE TO COME.\\nfinally haw saw a bigger chamber wid all the variety of cheese wid enormous quantity and was happy to find the rodents der enjoying the cheese. but this time haw was ready for the change observing the minute changes so tat he could act fast.\\n\\nAs days passed one fine day he heard the footsteps and guessed it was HEW and was happy finally he started searching for the new cheese..!!!\\n\\n(HAW could have gone to HEW and say about the new cheese at the chamber N. but he didnt since one cannot infuse the seeds to overcome the fear and accept the change . it should be from inself)\\n\\nso evryone out der.... let us\\nANTICIPATE CHANGE\\nADAPT TO CHANGE QUICKLY\\nENJOY THE CHANGE\\nBE READY TO CHANGE QUICKLY,AGAIN AND AGAIN\\n\\nSo who you are ??? SCURRY SNIFFY HEM OR HAW ??\\nAccepting changes like Scurry and Sniffy\\nor else learning about the advantages of accepting changes\\nor else hanging with the question... \" who moved my cheese??\"\\n\\nALL THE BEST.... :)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    html_free = soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \" \".join([c for c in text if c not in punctuations])\n",
    "    return no_punct\n",
    "#textclean = text_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "#instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#textclean = textclean.apply(lambda x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>who moved my cheese   the world has been devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday i attended a biweekly meeting of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam is nothing like natalie natalie never we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>in the eu we have browser choice but few know ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \n",
       "0   guest demo eric iversons itty bitty searchfeb...  \n",
       "1  who moved my cheese   the world has been devel...  \n",
       "2   yesterday i attended a biweekly meeting of an...  \n",
       "3   liam is nothing like natalie natalie never we...  \n",
       "4  in the eu we have browser choice but few know ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n",
    "data_clean = clean_text(text_df, 'text', 'text_clean')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \n",
       "0  guest demo eric iversons itty bitty searchfebr...  \n",
       "1  moved cheese world developing areas create dif...  \n",
       "2  yesterday attended biweekly meeting informal u...  \n",
       "3  liam nothing like natalie natalie never went d...  \n",
       "4  eu browser choice know eu tougher monopolies a...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk.corpus\n",
    "#nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guest demo eric iversons itty bitty searchfebruary th daniel tunkelangrespondim back vacation still digging way everything thats piled ive offlinewhile catch thought id share demo eric iverson gracious enough share uses yahoo boss support exploratory search experience top general web search enginewhen perform query application retrieves set related term candidates using yahoos key terms api scores term dividing occurrence count within result set global occurrence counta relevance measure similar one former colleagues used endeca enterprise contextsyou try demo rough edges produces nice resultsespecially considering simplicity approachheres example used application explore learn something new started information retrieval noticed interactive information retrieval top term used refine refinement suggestions looked familiar mebut unfamiliar name caught attention anton leuski following curiosity refined looking results immediately saw leuski done work evaluating document clustering interactive information retrieval exploration made clear someone whose work get knowcheck home pagei cant promise youll productive experience encourage try erics demo simple examples like remind value pursuing hcir open webspeaking hcir works well flesh details next weeks course ill share'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['text_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...  \n",
       "1  [move, chees, world, develop, area, creat, dif...  \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...  \n",
       "3  [liam, noth, like, natali, natali, never, went...  \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk \n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "\n",
    "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "      <td>[moved, cheese, world, developing, area, creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...   \n",
       "1  [move, chees, world, develop, area, creat, dif...   \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...   \n",
       "3  [liam, noth, like, natali, natali, never, went...   \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...   \n",
       "\n",
       "                                   text_tokens_lemma  \n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...  \n",
       "1  [moved, cheese, world, developing, area, creat...  \n",
       "2  [yesterday, attended, biweekly, meeting, infor...  \n",
       "3  [liam, nothing, like, natalie, natalie, never,...  \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hoangvo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guest Demo: Eric Iverson’s Itty Bitty Search\\...</td>\n",
       "      <td>M</td>\n",
       "      <td>guest demo eric iversons itty bitty searchfebr...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[guest, demo, eric, iverson, itti, bitti, sear...</td>\n",
       "      <td>[guest, demo, eric, iversons, itty, bitty, sea...</td>\n",
       "      <td>[(guest, JJS), (demo, NN), (eric, JJ), (iverso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who moved my Cheese???   The world has been de...</td>\n",
       "      <td>M</td>\n",
       "      <td>moved cheese world developing areas create dif...</td>\n",
       "      <td>[moved, cheese, world, developing, areas, crea...</td>\n",
       "      <td>[move, chees, world, develop, area, creat, dif...</td>\n",
       "      <td>[moved, cheese, world, developing, area, creat...</td>\n",
       "      <td>[(moved, VBN), (cheese, JJ), (world, NN), (dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yesterday I attended a biweekly meeting of an...</td>\n",
       "      <td>M</td>\n",
       "      <td>yesterday attended biweekly meeting informal u...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[yesterday, attend, biweekli, meet, inform, uc...</td>\n",
       "      <td>[yesterday, attended, biweekly, meeting, infor...</td>\n",
       "      <td>[(yesterday, NN), (attended, VBD), (biweekly, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liam is nothing like Natalie. Natalie never w...</td>\n",
       "      <td>F</td>\n",
       "      <td>liam nothing like natalie natalie never went d...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[liam, noth, like, natali, natali, never, went...</td>\n",
       "      <td>[liam, nothing, like, natalie, natalie, never,...</td>\n",
       "      <td>[(liam, RB), (nothing, NN), (like, IN), (natal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the EU we have browser choice, but few know...</td>\n",
       "      <td>M</td>\n",
       "      <td>eu browser choice know eu tougher monopolies a...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[eu, browser, choic, know, eu, tougher, monopo...</td>\n",
       "      <td>[eu, browser, choice, know, eu, tougher, monop...</td>\n",
       "      <td>[(eu, RB), (browser, NN), (choice, NN), (know,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0   Guest Demo: Eric Iverson’s Itty Bitty Search\\...     M   \n",
       "1  Who moved my Cheese???   The world has been de...     M   \n",
       "2   Yesterday I attended a biweekly meeting of an...     M   \n",
       "3   Liam is nothing like Natalie. Natalie never w...     F   \n",
       "4  In the EU we have browser choice, but few know...     M   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  guest demo eric iversons itty bitty searchfebr...   \n",
       "1  moved cheese world developing areas create dif...   \n",
       "2  yesterday attended biweekly meeting informal u...   \n",
       "3  liam nothing like natalie natalie never went d...   \n",
       "4  eu browser choice know eu tougher monopolies a...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, areas, crea...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                                    text_tokens_stem  \\\n",
       "0  [guest, demo, eric, iverson, itti, bitti, sear...   \n",
       "1  [move, chees, world, develop, area, creat, dif...   \n",
       "2  [yesterday, attend, biweekli, meet, inform, uc...   \n",
       "3  [liam, noth, like, natali, natali, never, went...   \n",
       "4  [eu, browser, choic, know, eu, tougher, monopo...   \n",
       "\n",
       "                                   text_tokens_lemma  \\\n",
       "0  [guest, demo, eric, iversons, itty, bitty, sea...   \n",
       "1  [moved, cheese, world, developing, area, creat...   \n",
       "2  [yesterday, attended, biweekly, meeting, infor...   \n",
       "3  [liam, nothing, like, natalie, natalie, never,...   \n",
       "4  [eu, browser, choice, know, eu, tougher, monop...   \n",
       "\n",
       "                              text_tokens_pos_tagged  \n",
       "0  [(guest, JJS), (demo, NN), (eric, JJ), (iverso...  \n",
       "1  [(moved, VBN), (cheese, JJ), (world, NN), (dev...  \n",
       "2  [(yesterday, NN), (attended, VBD), (biweekly, ...  \n",
       "3  [(liam, RB), (nothing, NN), (like, IN), (natal...  \n",
       "4  [(eu, RB), (browser, NN), (choice, NN), (know,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of speech tagging and chunking\n",
    "def word_pos_tagger(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    return pos_tagged_text\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_clean['text_tokens_pos_tagged'] = data_clean['text_tokens'].apply(lambda x: word_pos_tagger(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_stem</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "      <th>text_tokens_pos_tagged</th>\n",
       "      <th>fine</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>Wake Up Call is a free short story posted by ...</td>\n",
       "      <td>M</td>\n",
       "      <td>wake call free short story posted insomniac ho...</td>\n",
       "      <td>[wake, call, free, short, story, posted, insom...</td>\n",
       "      <td>[wake, call, free, short, stori, post, insomni...</td>\n",
       "      <td>[wake, call, free, short, story, posted, insom...</td>\n",
       "      <td>[(wake, VB), (call, JJ), (free, JJ), (short, J...</td>\n",
       "      <td>wake call free short story posted insomniac ho...</td>\n",
       "      <td>19850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Coast has become one of the best sushi spots i...</td>\n",
       "      <td>M</td>\n",
       "      <td>coast become one best sushi spots town longer ...</td>\n",
       "      <td>[coast, become, one, best, sushi, spots, town,...</td>\n",
       "      <td>[coast, becom, one, best, sushi, spot, town, l...</td>\n",
       "      <td>[coast, become, one, best, sushi, spot, town, ...</td>\n",
       "      <td>[(coast, NN), (become, VB), (one, CD), (best, ...</td>\n",
       "      <td>coast become best sushi spot town longer folk ...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Well, first of all, I have been sick with some...</td>\n",
       "      <td>F</td>\n",
       "      <td>well first sick sort nasty combination upper r...</td>\n",
       "      <td>[well, first, sick, sort, nasty, combination, ...</td>\n",
       "      <td>[well, first, sick, sort, nasti, combin, upper...</td>\n",
       "      <td>[well, first, sick, sort, nasty, combination, ...</td>\n",
       "      <td>[(well, RB), (first, RB), (sick, JJ), (sort, N...</td>\n",
       "      <td>well first sick sort nasty combination upper r...</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Achu hmmm first thing that comes to mind is di...</td>\n",
       "      <td>M</td>\n",
       "      <td>achu hmmm first thing comes mind discipline al...</td>\n",
       "      <td>[achu, hmmm, first, thing, comes, mind, discip...</td>\n",
       "      <td>[achu, hmmm, first, thing, come, mind, discipl...</td>\n",
       "      <td>[achu, hmmm, first, thing, come, mind, discipl...</td>\n",
       "      <td>[(achu, NN), (hmmm, NN), (first, JJ), (thing, ...</td>\n",
       "      <td>achu hmmm first thing come mind discipline alw...</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>\\nDo not be so dejected.This is the layer whic...</td>\n",
       "      <td>M</td>\n",
       "      <td>dejectedthis layer created himiemr gadkarithis...</td>\n",
       "      <td>[dejectedthis, layer, created, himiemr, gadkar...</td>\n",
       "      <td>[dejectedthi, layer, creat, himiemr, gadkarith...</td>\n",
       "      <td>[dejectedthis, layer, created, himiemr, gadkar...</td>\n",
       "      <td>[(dejectedthis, NN), (layer, NN), (created, VB...</td>\n",
       "      <td>dejectedthis layer created himiemr gadkarithis...</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>a long time coming? Spring! Luckily spring is ...</td>\n",
       "      <td>F</td>\n",
       "      <td>long time coming spring luckily spring air blo...</td>\n",
       "      <td>[long, time, coming, spring, luckily, spring, ...</td>\n",
       "      <td>[long, time, come, spring, luckili, spring, ai...</td>\n",
       "      <td>[long, time, coming, spring, luckily, spring, ...</td>\n",
       "      <td>[(long, JJ), (time, NN), (coming, VBG), (sprin...</td>\n",
       "      <td>long time coming spring luckily spring bloom a...</td>\n",
       "      <td>1685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>OK March 1st and now a new mind set.  At least...</td>\n",
       "      <td>F</td>\n",
       "      <td>ok march st new mind set least trying focus po...</td>\n",
       "      <td>[ok, march, st, new, mind, set, least, trying,...</td>\n",
       "      <td>[ok, march, st, new, mind, set, least, tri, fo...</td>\n",
       "      <td>[ok, march, st, new, mind, set, least, trying,...</td>\n",
       "      <td>[(ok, JJ), (march, NN), (st, VB), (new, JJ), (...</td>\n",
       "      <td>march mind least trying focus positiveit past ...</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>I've only ever really had like maybe 7 people ...</td>\n",
       "      <td>M</td>\n",
       "      <td>ive ever really like maybe people actually rea...</td>\n",
       "      <td>[ive, ever, really, like, maybe, people, actua...</td>\n",
       "      <td>[ive, ever, realli, like, mayb, peopl, actual,...</td>\n",
       "      <td>[ive, ever, really, like, maybe, people, actua...</td>\n",
       "      <td>[(ive, JJ), (ever, RB), (really, RB), (like, J...</td>\n",
       "      <td>ever really like maybe people actually read st...</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>My husband was supposed to be at work at 1:30 ...</td>\n",
       "      <td>F</td>\n",
       "      <td>husband supposed work pm today ended late near...</td>\n",
       "      <td>[husband, supposed, work, pm, today, ended, la...</td>\n",
       "      <td>[husband, suppos, work, pm, today, end, late, ...</td>\n",
       "      <td>[husband, supposed, work, pm, today, ended, la...</td>\n",
       "      <td>[(husband, NN), (supposed, VBD), (work, NN), (...</td>\n",
       "      <td>husband supposed work today ended late nearly ...</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>hey :) Cool questions, is this like a test? I ...</td>\n",
       "      <td>M</td>\n",
       "      <td>hey cool questions like test would say guys pr...</td>\n",
       "      <td>[hey, cool, questions, like, test, would, say,...</td>\n",
       "      <td>[hey, cool, question, like, test, would, say, ...</td>\n",
       "      <td>[hey, cool, question, like, test, would, say, ...</td>\n",
       "      <td>[(hey, JJ), (cool, VBP), (questions, NNS), (li...</td>\n",
       "      <td>cool question like test would probably lose we...</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Argentina has just approved chemical castratio...</td>\n",
       "      <td>F</td>\n",
       "      <td>argentina approved chemical castration convict...</td>\n",
       "      <td>[argentina, approved, chemical, castration, co...</td>\n",
       "      <td>[argentina, approv, chemic, castrat, convict, ...</td>\n",
       "      <td>[argentina, approved, chemical, castration, co...</td>\n",
       "      <td>[(argentina, RB), (approved, JJ), (chemical, J...</td>\n",
       "      <td>argentina approved chemical castration convict...</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>Last week my girl's &amp; I found out some very sa...</td>\n",
       "      <td>F</td>\n",
       "      <td>last week girls found sad news cat friend pass...</td>\n",
       "      <td>[last, week, girls, found, sad, news, cat, fri...</td>\n",
       "      <td>[last, week, girl, found, sad, news, cat, frie...</td>\n",
       "      <td>[last, week, girl, found, sad, news, cat, frie...</td>\n",
       "      <td>[(last, JJ), (week, NN), (girls, NNS), (found,...</td>\n",
       "      <td>last week girl found news friend passed away p...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>I love Continental Airlines! They never disapp...</td>\n",
       "      <td>F</td>\n",
       "      <td>love continental airlines never disappoint thr...</td>\n",
       "      <td>[love, continental, airlines, never, disappoin...</td>\n",
       "      <td>[love, continent, airlin, never, disappoint, t...</td>\n",
       "      <td>[love, continental, airline, never, disappoint...</td>\n",
       "      <td>[(love, VB), (continental, NN), (airlines, NNS...</td>\n",
       "      <td>love continental airline never disappoint thri...</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>Today is Valentine's Day. There are many custo...</td>\n",
       "      <td>M</td>\n",
       "      <td>today valentines day many customs valentines d...</td>\n",
       "      <td>[today, valentines, day, many, customs, valent...</td>\n",
       "      <td>[today, valentin, day, mani, custom, valentin,...</td>\n",
       "      <td>[today, valentine, day, many, custom, valentin...</td>\n",
       "      <td>[(today, NN), (valentines, NNS), (day, NN), (m...</td>\n",
       "      <td>today valentine many custom valentine world co...</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>I grew up on a farm outside Cazenovia, NY and ...</td>\n",
       "      <td>M</td>\n",
       "      <td>grew farm outside cazenovia ny attended harvar...</td>\n",
       "      <td>[grew, farm, outside, cazenovia, ny, attended,...</td>\n",
       "      <td>[grew, farm, outsid, cazenovia, ny, attend, ha...</td>\n",
       "      <td>[grew, farm, outside, cazenovia, ny, attended,...</td>\n",
       "      <td>[(grew, VBD), (farm, NN), (outside, IN), (caze...</td>\n",
       "      <td>grew farm outside cazenovia attended harvard c...</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text class  \\\n",
       "1292   Wake Up Call is a free short story posted by ...     M   \n",
       "1638  Coast has become one of the best sushi spots i...     M   \n",
       "1582  Well, first of all, I have been sick with some...     F   \n",
       "1088  Achu hmmm first thing that comes to mind is di...     M   \n",
       "2631  \\nDo not be so dejected.This is the layer whic...     M   \n",
       "273   a long time coming? Spring! Luckily spring is ...     F   \n",
       "1812  OK March 1st and now a new mind set.  At least...     F   \n",
       "1487  I've only ever really had like maybe 7 people ...     M   \n",
       "1964  My husband was supposed to be at work at 1:30 ...     F   \n",
       "960   hey :) Cool questions, is this like a test? I ...     M   \n",
       "457   Argentina has just approved chemical castratio...     F   \n",
       "2953  Last week my girl's & I found out some very sa...     F   \n",
       "2917  I love Continental Airlines! They never disapp...     F   \n",
       "1225  Today is Valentine's Day. There are many custo...     M   \n",
       "1963  I grew up on a farm outside Cazenovia, NY and ...     M   \n",
       "\n",
       "                                             text_clean  \\\n",
       "1292  wake call free short story posted insomniac ho...   \n",
       "1638  coast become one best sushi spots town longer ...   \n",
       "1582  well first sick sort nasty combination upper r...   \n",
       "1088  achu hmmm first thing comes mind discipline al...   \n",
       "2631  dejectedthis layer created himiemr gadkarithis...   \n",
       "273   long time coming spring luckily spring air blo...   \n",
       "1812  ok march st new mind set least trying focus po...   \n",
       "1487  ive ever really like maybe people actually rea...   \n",
       "1964  husband supposed work pm today ended late near...   \n",
       "960   hey cool questions like test would say guys pr...   \n",
       "457   argentina approved chemical castration convict...   \n",
       "2953  last week girls found sad news cat friend pass...   \n",
       "2917  love continental airlines never disappoint thr...   \n",
       "1225  today valentines day many customs valentines d...   \n",
       "1963  grew farm outside cazenovia ny attended harvar...   \n",
       "\n",
       "                                            text_tokens  \\\n",
       "1292  [wake, call, free, short, story, posted, insom...   \n",
       "1638  [coast, become, one, best, sushi, spots, town,...   \n",
       "1582  [well, first, sick, sort, nasty, combination, ...   \n",
       "1088  [achu, hmmm, first, thing, comes, mind, discip...   \n",
       "2631  [dejectedthis, layer, created, himiemr, gadkar...   \n",
       "273   [long, time, coming, spring, luckily, spring, ...   \n",
       "1812  [ok, march, st, new, mind, set, least, trying,...   \n",
       "1487  [ive, ever, really, like, maybe, people, actua...   \n",
       "1964  [husband, supposed, work, pm, today, ended, la...   \n",
       "960   [hey, cool, questions, like, test, would, say,...   \n",
       "457   [argentina, approved, chemical, castration, co...   \n",
       "2953  [last, week, girls, found, sad, news, cat, fri...   \n",
       "2917  [love, continental, airlines, never, disappoin...   \n",
       "1225  [today, valentines, day, many, customs, valent...   \n",
       "1963  [grew, farm, outside, cazenovia, ny, attended,...   \n",
       "\n",
       "                                       text_tokens_stem  \\\n",
       "1292  [wake, call, free, short, stori, post, insomni...   \n",
       "1638  [coast, becom, one, best, sushi, spot, town, l...   \n",
       "1582  [well, first, sick, sort, nasti, combin, upper...   \n",
       "1088  [achu, hmmm, first, thing, come, mind, discipl...   \n",
       "2631  [dejectedthi, layer, creat, himiemr, gadkarith...   \n",
       "273   [long, time, come, spring, luckili, spring, ai...   \n",
       "1812  [ok, march, st, new, mind, set, least, tri, fo...   \n",
       "1487  [ive, ever, realli, like, mayb, peopl, actual,...   \n",
       "1964  [husband, suppos, work, pm, today, end, late, ...   \n",
       "960   [hey, cool, question, like, test, would, say, ...   \n",
       "457   [argentina, approv, chemic, castrat, convict, ...   \n",
       "2953  [last, week, girl, found, sad, news, cat, frie...   \n",
       "2917  [love, continent, airlin, never, disappoint, t...   \n",
       "1225  [today, valentin, day, mani, custom, valentin,...   \n",
       "1963  [grew, farm, outsid, cazenovia, ny, attend, ha...   \n",
       "\n",
       "                                      text_tokens_lemma  \\\n",
       "1292  [wake, call, free, short, story, posted, insom...   \n",
       "1638  [coast, become, one, best, sushi, spot, town, ...   \n",
       "1582  [well, first, sick, sort, nasty, combination, ...   \n",
       "1088  [achu, hmmm, first, thing, come, mind, discipl...   \n",
       "2631  [dejectedthis, layer, created, himiemr, gadkar...   \n",
       "273   [long, time, coming, spring, luckily, spring, ...   \n",
       "1812  [ok, march, st, new, mind, set, least, trying,...   \n",
       "1487  [ive, ever, really, like, maybe, people, actua...   \n",
       "1964  [husband, supposed, work, pm, today, ended, la...   \n",
       "960   [hey, cool, question, like, test, would, say, ...   \n",
       "457   [argentina, approved, chemical, castration, co...   \n",
       "2953  [last, week, girl, found, sad, news, cat, frie...   \n",
       "2917  [love, continental, airline, never, disappoint...   \n",
       "1225  [today, valentine, day, many, custom, valentin...   \n",
       "1963  [grew, farm, outside, cazenovia, ny, attended,...   \n",
       "\n",
       "                                 text_tokens_pos_tagged  \\\n",
       "1292  [(wake, VB), (call, JJ), (free, JJ), (short, J...   \n",
       "1638  [(coast, NN), (become, VB), (one, CD), (best, ...   \n",
       "1582  [(well, RB), (first, RB), (sick, JJ), (sort, N...   \n",
       "1088  [(achu, NN), (hmmm, NN), (first, JJ), (thing, ...   \n",
       "2631  [(dejectedthis, NN), (layer, NN), (created, VB...   \n",
       "273   [(long, JJ), (time, NN), (coming, VBG), (sprin...   \n",
       "1812  [(ok, JJ), (march, NN), (st, VB), (new, JJ), (...   \n",
       "1487  [(ive, JJ), (ever, RB), (really, RB), (like, J...   \n",
       "1964  [(husband, NN), (supposed, VBD), (work, NN), (...   \n",
       "960   [(hey, JJ), (cool, VBP), (questions, NNS), (li...   \n",
       "457   [(argentina, RB), (approved, JJ), (chemical, J...   \n",
       "2953  [(last, JJ), (week, NN), (girls, NNS), (found,...   \n",
       "2917  [(love, VB), (continental, NN), (airlines, NNS...   \n",
       "1225  [(today, NN), (valentines, NNS), (day, NN), (m...   \n",
       "1963  [(grew, VBD), (farm, NN), (outside, IN), (caze...   \n",
       "\n",
       "                                                   fine  clean_length  \n",
       "1292  wake call free short story posted insomniac ho...         19850  \n",
       "1638  coast become best sushi spot town longer folk ...           173  \n",
       "1582  well first sick sort nasty combination upper r...           390  \n",
       "1088  achu hmmm first thing come mind discipline alw...           519  \n",
       "2631  dejectedthis layer created himiemr gadkarithis...           419  \n",
       "273   long time coming spring luckily spring bloom a...          1685  \n",
       "1812  march mind least trying focus positiveit past ...           629  \n",
       "1487  ever really like maybe people actually read st...           279  \n",
       "1964  husband supposed work today ended late nearly ...           470  \n",
       "960   cool question like test would probably lose we...           325  \n",
       "457   argentina approved chemical castration convict...          1009  \n",
       "2953  last week girl found news friend passed away p...           207  \n",
       "2917  love continental airline never disappoint thri...           322  \n",
       "1225  today valentine many custom valentine world co...           569  \n",
       "1963  grew farm outside cazenovia attended harvard c...           711  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove short words less than 3\n",
    "data_clean['fine'] = data_clean['text_tokens_lemma'].apply(lambda x: ' '.join([w for w in x if len(w)>3]))\n",
    "# Count the length of characters\n",
    "data_clean['clean_length'] = data_clean['fine'].apply(len)\n",
    "# Remove rows where character length <= 5\n",
    "data_clean = data_clean[data_clean.clean_length > 5]\n",
    "data_clean.sample(n=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3223, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(text):\n",
    "    #clean text\n",
    "    newText = text.str.lower()\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
    "    newText = newText.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    #stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    newText = newText.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #stemmer\n",
    "    newText = newText.apply(lambda x: word_tokenize(x))\n",
    "    newText_stem = newText.apply(lambda x: word_stemmer(x))\n",
    "    #token lemma\n",
    "    newText_lem = newText.apply(lambda x: word_lemmatizer(x))\n",
    "    newText_tag = newText.apply(lambda x: word_pos_tagger(x))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare X and Y data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pub.towardsai.net/does-a-machine-know-your-gender-based-on-your-tweets-43b14740fd54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaaahs</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aacross</th>\n",
       "      <th>aada</th>\n",
       "      <th>aadi</th>\n",
       "      <th>...</th>\n",
       "      <th>zwei</th>\n",
       "      <th>zwierzynski</th>\n",
       "      <th>zydecodancing</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zyprexaand</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zzzzs</th>\n",
       "      <th>zzzzzs</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3223 rows × 58567 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaa  aaaaah  aaaaand  aaaah  aaaahs  aaaand  aaah  aacross  aada  aadi  \\\n",
       "0        0       0        0      0       0       0     0        0     0     0   \n",
       "1        0       0        0      0       0       0     0        0     0     0   \n",
       "2        0       0        0      0       0       0     0        0     0     0   \n",
       "3        0       0        0      0       0       0     0        0     0     0   \n",
       "4        0       0        0      0       0       0     0        0     0     0   \n",
       "...    ...     ...      ...    ...     ...     ...   ...      ...   ...   ...   \n",
       "3218     0       0        0      0       0       0     0        0     0     0   \n",
       "3219     0       0        0      0       0       0     0        0     0     0   \n",
       "3220     0       0        0      0       0       0     0        0     0     0   \n",
       "3221     0       0        0      0       0       0     0        0     0     0   \n",
       "3222     0       0        0      0       0       0     0        0     0     0   \n",
       "\n",
       "      ...  zwei  zwierzynski  zydecodancing  zynga  zyprexaand  zyrtec  zzzzs  \\\n",
       "0     ...     0            0              0      0           0       0      0   \n",
       "1     ...     0            0              0      0           0       0      0   \n",
       "2     ...     0            0              0      0           0       0      0   \n",
       "3     ...     0            0              0      0           0       0      0   \n",
       "4     ...     0            0              0      0           0       0      0   \n",
       "...   ...   ...          ...            ...    ...         ...     ...    ...   \n",
       "3218  ...     0            0              0      0           0       0      0   \n",
       "3219  ...     0            0              0      0           0       0      0   \n",
       "3220  ...     0            0              0      0           0       0      0   \n",
       "3221  ...     0            0              0      0           0       0      0   \n",
       "3222  ...     0            0              0      0           0       0      0   \n",
       "\n",
       "      zzzzzs  zzzzzzzzzzzzzzzzzz  \\\n",
       "0          0                   0   \n",
       "1          0                   0   \n",
       "2          0                   0   \n",
       "3          0                   0   \n",
       "4          0                   0   \n",
       "...      ...                 ...   \n",
       "3218       0                   0   \n",
       "3219       0                   0   \n",
       "3220       0                   0   \n",
       "3221       0                   0   \n",
       "3222       0                   0   \n",
       "\n",
       "      zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                                     0                        \n",
       "1                                                     0                        \n",
       "2                                                     0                        \n",
       "3                                                     0                        \n",
       "4                                                     0                        \n",
       "...                                                 ...                        \n",
       "3218                                                  0                        \n",
       "3219                                                  0                        \n",
       "3220                                                  0                        \n",
       "3221                                                  0                        \n",
       "3222                                                  0                        \n",
       "\n",
       "[3223 rows x 58567 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import f1_score\n",
    "# Bag-of-words features\n",
    "bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(data_clean['fine'])\n",
    "df_bow = pd.DataFrame(bow.todense(), columns=bow_vectorizer.get_feature_names())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/anaconda3/envs/Data-mining/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_clean['gender'] = data_clean['class'].apply(lambda x: 1 if x=='F' else 0)\n",
    "# Splitting the data into training and test set\n",
    "X = df_bow\n",
    "y = data_clean['gender']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6943999999999999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting on Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "prediction_bow = logreg.predict_proba(X_test)\n",
    "# Calculating the F1 score\n",
    "# If prediction is greater than or equal to 0.5 than 1, else 0\n",
    "# Gender, 0 = male and 1 = female\n",
    "prediction_int = prediction_bow[:,1]>=0.5\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "# Calculating F1 score\n",
    "log_bow = f1_score(y_test, prediction_int)\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703875968992248\n"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237 106]\n",
      " [ 85 217]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6976744186046512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB()),\n",
    " #                      ])\n",
    "#text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted = text_clf.predict(X_test)\n",
    "#np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6945736434108527"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter_no_change=5, random_state=42)\n",
    "_ = clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = text_df[\"text\"]\n",
    "y1 = text_df[\"class\"].apply(lambda x: 1 if x=='F' else 0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209302325581395"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    " ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train2, y_train2)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test2)\n",
    "np.mean(predicted_mnb_stemmed == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Neuron Network: https://realpython.com/python-keras-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
